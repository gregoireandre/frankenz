{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of FRANKEN-Z to Real Data\n",
    "\n",
    "This notebook demonstrates an application of FRANKEN-Z to S16A **Hyper Suprime-Cam (HSC) Subaru Strategic Program (SSP) survey data**. As the training data includes $\\sim\\!400,000$ objects that have been **inhomogeneously sampled** (cobbled together from various surveys) with **varying errors** (stochastic backgrounds) and **depths** (collected from wide, deep, and ultra-deep fields) along with **multiple flux measurements** (PSF, Cmodel, and fiber/aperture quantities) and **randomly censored data** (from failed extractions and missing coverage), modeling these data provides many new challenges and opportunities.\n",
    "\n",
    "This fourth notebook outlines an applications to HSC COSMOS observations observed in different seeing conditions to quantify how performance changes as a function of seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import sys # system commands\n",
    "import gc # garbage memory collection\n",
    "from scipy import stats # statistics\n",
    "from scipy import special # special functions\n",
    "from astropy.io import fits # reading fits files\n",
    "\n",
    "# FRANKEN-Z\n",
    "import frankenz as fz\n",
    "\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "# re-defining plotting defaults\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'xtick.color': 'k'})\n",
    "rcParams.update({'ytick.color': 'k'})\n",
    "rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing FRANKEN-Z config files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# master config file\n",
    "config=fz.ReadParams('config/frankenz.config') \n",
    "\n",
    "# import filters\n",
    "filt=fz.ReadFilters(config.params['FILTERS'],path=config.params['FILTER_PATH'])\n",
    "Nf=filt.NFILTER\n",
    "\n",
    "# initialize redshift dictionary\n",
    "rdict=fz.RedshiftDict(config.CONFIG_REDSHIFT.params)\n",
    "\n",
    "# add names so easier to reference later\n",
    "rdict.sig_dict=rdict.lze_dict\n",
    "rdict.sig_width=rdict.lze_width\n",
    "rdict.delta=rdict.dlz_highres\n",
    "rdict.grid=rdict.lzgrid_highres\n",
    "rdict.Ngrid=rdict.Nz_highres\n",
    "\n",
    "# initialize supplementary dictionaries\n",
    "mdict=fz.PDFDict(config.CONFIG_MAG.params) # magnitude\n",
    "cdict=fz.PDFDict(config.CONFIG_COLOR.params) # color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing HSC training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training objects: 373432\n"
     ]
    }
   ],
   "source": [
    "# load observed data\n",
    "hdul=fits.open('/Users/Josh/Dropbox/HSC/HSC_photoz/catalogs/hsc_s16a_combined_specz_highq_clean_errsim_train_v1.fits')\n",
    "data=hdul[1].data\n",
    "print 'Training objects:',len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median correction: [ 0.94042301  0.95740581  0.96941328  0.97632635  0.97976404]\n"
     ]
    }
   ],
   "source": [
    "aphot=10**(-0.4*c_[data['a_g'],data['a_r'],data['a_i'],data['a_z'],data['a_y']])\n",
    "print 'Median correction:',median(aphot,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "flux_cmodel=c_[data['gcmodel_flux'],data['rcmodel_flux'],data['icmodel_flux'],data['zcmodel_flux'],data['ycmodel_flux']]*aphot\n",
    "err_cmodel=c_[data['gcmodel_flux_err'],data['rcmodel_flux_err'],data['icmodel_flux_err'],\n",
    "           data['zcmodel_flux_err'],data['ycmodel_flux_err']]*aphot\n",
    "err_cmodel_wide=c_[data['gcmodel_flux_err_wide'],data['rcmodel_flux_err_wide'],data['icmodel_flux_err_wide'],\n",
    "                data['zcmodel_flux_err_wide'],data['ycmodel_flux_err_wide']]*aphot\n",
    "mask_cmodel=(err_cmodel>0.)&isfinite(err_cmodel)\n",
    "print 'Objects with missing values:',(mask_cmodel.sum(axis=1)<Nf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects with missing values: 3\n"
     ]
    }
   ],
   "source": [
    "flux_afterburner=c_[data['gparent_flux_convolved_2_1'],data['rparent_flux_convolved_2_1'],data['iparent_flux_convolved_2_1'],\n",
    "                   data['zparent_flux_convolved_2_1'],data['yparent_flux_convolved_2_1']]*aphot\n",
    "err_afterburner=c_[data['gflux_aperture15_err'],data['rflux_aperture15_err'],data['iflux_aperture15_err'],\n",
    "                   data['zflux_aperture15_err'],data['yflux_aperture15_err']]*aphot\n",
    "\n",
    "mask_afterburner=(err_afterburner>0.)&isfinite(err_afterburner)\n",
    "print 'Objects with missing values:',(mask_afterburner.sum(axis=1)<Nf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z,ze,zt,zs=data['redshift'],data['redshift_err'],data['redshift_type'],data['redshift_source']\n",
    "ze[ze<0]=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ntrain=len(flux_afterburner) # number of objects\n",
    "flux_zeropoint=10**(-0.4*-23.9) # AB magnitude zeropoint\n",
    "skynoise=median(err_cmodel_wide,axis=0) # \"background\" skynoise (used for consistent mappings)\n",
    "mag_cmodel,magerr_cmodel=fz.asinh_mag_map(flux_cmodel,err_cmodel_wide,skynoise,zeropoint=flux_zeropoint) # Luptitude mapping\n",
    "mag_afterburner,magerr_afterburner=fz.asinh_mag_map(flux_afterburner,err_cmodel_wide,skynoise,zeropoint=flux_zeropoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lzidx,lzeidx=rdict.fit(log(1+z),ze/(1+z)) # discretize redshifts\n",
    "magidx,mageidx=mdict.fit(mag_cmodel,magerr_cmodel) # discretize magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clearing up memory\n",
    "del hdul[1].data,data\n",
    "for hdu in hdul:\n",
    "    del hdu\n",
    "del hdul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing HSC target data (**best/median/worst seeing**)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing objects: 154447\n"
     ]
    }
   ],
   "source": [
    "# load observed data\n",
    "seeing='worst'\n",
    "data=loadtxt('/Users/Josh/Dropbox/HSC/HSC_photoz/test_16a/cosmos_cat/matched_'+seeing+'_v1_ab.cat',dtype='str')\n",
    "print 'Testing objects:',len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median correction: [ 0.94516009  0.96082073  0.97187972  0.97824156  0.98140383]\n"
     ]
    }
   ],
   "source": [
    "aphot_test=10**(-0.4*data[:,[22,44,66,88,110]].astype('float32'))\n",
    "print 'Median correction:',median(aphot_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects with missing values: 1187\n"
     ]
    }
   ],
   "source": [
    "flux_afterburner_test=(data[:,[9,31,53,75,97]].astype('float32'))*aphot_test*1e29\n",
    "err_afterburner_test=(data[:,[10,32,54,76,98]].astype('float32'))*aphot_test*1e29\n",
    "mask_afterburner_test=(err_afterburner_test>0.)&isfinite(err_afterburner_test)\n",
    "mask_afterburner_test*=(flux_afterburner_test!=0.)&isfinite(flux_afterburner_test)\n",
    "print 'Objects with missing values:',(mask_afterburner_test.sum(axis=1)<Nf).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data (removing objects with no measured fluxes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects with no flux measurements: 0\n"
     ]
    }
   ],
   "source": [
    "flux_sel=mask_afterburner_test.sum(axis=1)>0\n",
    "print 'Objects with no flux measurements:',(flux_sel==False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Nobs=flux_sel.sum() # number of objects\n",
    "mag_afterburner_test,magerr_afterburner_test=fz.asinh_mag_map(flux_afterburner_test,err_afterburner_test,\n",
    "                                                              skynoise,zeropoint=flux_zeropoint)\n",
    "magidx_test,mageidx_test=mdict.fit(mag_afterburner_test,magerr_afterburner_test) # discretize magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clearing up memory\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our fluxes, errors, and masks along with our WINBET instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# redefining fluxes\n",
    "\n",
    "p1,v1,m1=flux_afterburner,square(err_afterburner),mask_afterburner # training\n",
    "p1[m1==False],v1[m1==False]=1.,1. # filling in missing values with arbitrary values\n",
    "p2,v2,m2=flux_afterburner_test,square(err_afterburner_test),mask_afterburner_test # testing (observed)\n",
    "p2[m2==False],v2[m2==False]=1.,1. # filling in missing values with arbitrary values\n",
    "e1,e2=sqrt(v1+square(0.01*p1)),sqrt(v2+square(0.01*p2)) # add 1% error floor\n",
    "\n",
    "mag1,mage1=fz.asinh_mag_map(p1,e1,zeropoint=flux_zeropoint,skynoise=skynoise) # Luptitude mapping\n",
    "mag2,mage2=fz.asinh_mag_map(p2,e2,zeropoint=flux_zeropoint,skynoise=skynoise) # Luptitude mapping\n",
    "\n",
    "del flux_afterburner,err_afterburner,mask_afterburner\n",
    "del flux_afterburner_test,err_afterburner_test,mask_afterburner_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 "
     ]
    }
   ],
   "source": [
    "N_members=25 # number of trees/Monte Carlo realizations used for nearest-neighbor search\n",
    "N_neighbors=10 # number of neighbors selected at each iteration\n",
    "\n",
    "winbet_train=fz.WINBET(Ntrees=N_members,Nleaf=N_neighbors)\n",
    "winbet_test=fz.WINBET(Ntrees=N_members,Nleaf=N_neighbors)\n",
    "if (m1==False).sum()>0:\n",
    "    winbet_train.train(p1,v1,m1,mag1,mage1,mdict)\n",
    "else:\n",
    "    winbet_train=None\n",
    "if (m2==False).sum()>0:\n",
    "    winbet_test.train(p2[flux_sel],v2[flux_sel],m2[flux_sel],mag2[flux_sel],mage2[flux_sel],mdict)\n",
    "else:\n",
    "    winbet_test=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this all set, let's fit the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0 0 500 1000 1: 0 0 500 1000 2: 0 0 500 1000 3: 0 0 500 1000 4: 0 0 500 1000 5: 0 0 500 1000 6: 0 0 500 1000 7: 0 0 500 1000 8: 0 0 500 1000 9: 0 0 500 1000 10: 0 0 500 1000 11: 0 0 500 1000 12: 0 0 500 1000 13: 0 0 500 1000 14: 0 0 500 1000 15: 0 0 500 1000 16: 0 0 500 1000 17: 0 0 500 1000 18: 0 0 500 1000 19: 0 0 500 1000 20: 0 0 500 1000 21: 0 0 500 1000 22: 0 0 500 1000 23: 0 0 500 1000 24: 0 0 500 1000 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 55000 60000 65000 70000 75000 80000 85000 90000 95000 100000 105000 110000 115000 120000 125000 130000 135000 140000 145000 150000 done!\n"
     ]
    }
   ],
   "source": [
    "frankenz=fz.FRANKENZ(N_members=N_members,n_neighbors=N_neighbors)\n",
    "model_obj,model_Nobj,model_ll,model_Nbands=frankenz.predict(p1,e1,m1,\n",
    "                                                            p2[flux_sel],e2[flux_sel],m2[flux_sel],\n",
    "                                                            impute_train=winbet_train,impute_test=winbet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Redshift PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 55000 60000 65000 70000 75000 80000 85000 90000 95000 100000 105000 110000 115000 120000 125000 130000 135000 140000 145000 150000 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 55000 60000 65000 70000 75000 80000 "
     ]
    }
   ],
   "source": [
    "# generate redshifts\n",
    "model_llmin=empty(Nobs,dtype='float32') # min(ln-likelihood)\n",
    "lzpdf=empty((Nobs,rdict.Nz),dtype='float32') # ln(1+z) PDF\n",
    "zpdf=empty((Nobs,rdict.Nz_out),dtype='float32') # z PDF\n",
    "model_levidence=empty(Nobs,dtype='float32') # -2ln(evidence)\n",
    "\n",
    "for i in xrange(Nobs):\n",
    "    if i%5000==0: sys.stdout.write(str(i)+' ')\n",
    "    Nm=model_Nobj[i]\n",
    "    midx,ll=model_obj[i][:Nm],model_ll[i][:Nm]\n",
    "    model_llmin[i]=ll.min() # minimum value (for scaling)\n",
    "    w=exp(-0.5*(ll-model_llmin[i])) # transform to scaled likelihood weights\n",
    "    model_levidence[i]=-2*log(w.sum())+model_llmin[i] # -2ln(Evidence)\n",
    "    \n",
    "    pz=fz.pdf_kde_dict(rdict.sig_dict,rdict.sig_width,lzidx[midx],lzeidx[midx],w,\n",
    "                       rdict.grid,rdict.delta,rdict.Ngrid) # KDE dictionary PDF\n",
    "    lzpdf[i]=pz[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]\n",
    "    \n",
    "# compute Ntype and Ptype\n",
    "model_Ntype=zeros((Nobs,3),dtype='float32')\n",
    "model_Ptype=zeros((Nobs,3),dtype='float32')\n",
    "\n",
    "for i in xrange(Nobs):\n",
    "    if i%5000==0: sys.stdout.write(str(i)+' ')\n",
    "    Nm=model_Nobj[i]\n",
    "    midx,ll=model_obj[i][:Nm],model_ll[i][:Nm]\n",
    "    ztypes=zt[midx]-1\n",
    "    like=exp(-0.5*(ll-model_llmin[i]))\n",
    "    like/=like.sum()\n",
    "    for j in xrange(Nm):\n",
    "        model_Ntype[i][ztypes[j]]+=1\n",
    "        model_Ptype[i][ztypes[j]]+=like[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sel=arange(len(p2))[flux_sel]\n",
    "zpdf=fz.pdfs_resample(rdict.zgrid,lzpdf/rdict.znorm,rdict.zgrid_out) # resample from ln(1+z) to z space\n",
    "del lzpdf\n",
    "gc.collect()\n",
    "\n",
    "cols=fits.ColDefs([\n",
    "        fits.Column(name='model_sel',format='J',array=model_sel),\n",
    "        fits.Column(name='model_llmin',format='E',array=model_llmin),\n",
    "        fits.Column(name='model_levidence',format='E',array=model_levidence),\n",
    "        fits.Column(name='model_Ntype',format='3E',array=model_Ntype),\n",
    "        fits.Column(name='model_Ptype',format='3E',array=model_Ptype),\n",
    "        fits.Column(name='zpdf',format='601E',array=zpdf)\n",
    "        ])\n",
    "tbhdu=fits.BinTableHDU.from_columns(cols)\n",
    "tbhdu.writeto('/Users/Josh/Dropbox/HSC/HSC_photoz/test_16a/hsc_photoz_s16a_cosmos_'+seeing+'_zpdf.fits',clobber=True)\n",
    "del cols,tbhdu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Redshift Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have the \"true\" results, we only can quantify broad features such as, e.g., photo-z dispersion about the median solution, the $dN/dz$ distribution, etc. Let's start with the $dN/dz$ solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,6))\n",
    "\n",
    "# training\n",
    "z_pdf=fz.pdf_kde_dict(rdict.sig_dict,rdict.sig_width,lzidx,lzeidx,ones(Ntrain),rdict.grid,rdict.delta,rdict.Ngrid)\n",
    "z_pdf=z_pdf[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]/rdict.znorm\n",
    "z_pdf=interp(rdict.zgrid_out,rdict.zgrid,z_pdf)\n",
    "z_pdf/=z_pdf.sum()\n",
    "plot(rdict.zgrid_out,z_pdf,lw=5,color='black',label='Training')\n",
    "\n",
    "nz_stack=nansum(zpdf,axis=0)/Nobs\n",
    "plot(rdict.zgrid_out,nz_stack,lw=5,color='red',label='Predicted')\n",
    "xlim([rdict.zgrid_out.min(),rdict.zgrid_out.max()])\n",
    "legend(fontsize=20)\n",
    "tight_layout()\n",
    "xlabel('Redshift')\n",
    "yticks([])\n",
    "ylabel('dN/dz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That broadly looks reasonable. Now let's examine the error properties as a function of magnitude. Plotted below is the PDF dispersion about the median as a function of Cmodel magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute point estimates\n",
    "zpoints=fz.pdfs_summary_statistics(rdict.zgrid_out,zpdf) # mean, med, mode, l68, h68, l95, h95, std, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,8))\n",
    "mzstack,mzprob,mzpoints=fz.plot_dpdfstack(zpdf,rdict.zgrid_out,zpoints[1],\n",
    "                                          magidx_test[:,2][model_sel],mageidx_test[:,2][model_sel],\n",
    "                                          mdict,[18,28,1.],'i_mag',\n",
    "                                          sel=m2[:,2][model_sel],boxcar=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a heterogeneous sample, it's also useful to check how well we're performing on each given redshift type. The distribution as a function of magnitude and performance in several $p_t$ bins are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,10))\n",
    "subplot(3,2,1)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ntype[:,0]/model_Ntype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('F_spec')\n",
    "tight_layout()\n",
    "subplot(3,2,2)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ptype[:,0]/model_Ptype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('P_spec')\n",
    "tight_layout()\n",
    "subplot(3,2,3)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ntype[:,1]/model_Ntype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('F_p/grism')\n",
    "tight_layout()\n",
    "subplot(3,2,4)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ptype[:,1]/model_Ptype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('P_p/grism')\n",
    "tight_layout()\n",
    "subplot(3,2,5)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ntype[:,2]/model_Ntype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('F_phot')\n",
    "tight_layout()\n",
    "subplot(3,2,6)\n",
    "h=hist2d(mag_afterburner_test[:,2][model_sel],model_Ptype[:,2]/model_Ptype.sum(axis=1),\n",
    "         bins=[arange(10,30,0.05),linspace(0,1,200)],norm=matplotlib.colors.LogNorm(),cmin=1)\n",
    "colorbar()\n",
    "xlim([18,28])\n",
    "ylim([0,1])\n",
    "yticks([0,0.5,1.])\n",
    "xlabel('i_mag')\n",
    "ylabel('P_phot')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, photo-z's tend to dominate the neighbors and likelihood fraction at faint magnitudes, with a sharp transition between 22nd and 24th mag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming our spec-z and grism-z based predictions are \"robust\", we can use $f_t$ and $p_t$ to give us some proxy into the error properties of our sample as we transition away from \"confident\" redshifts into more speculative regions of color-magnitude space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,8))\n",
    "sel=model_Ptype[:,2]<0.25\n",
    "fsel=round(sel.sum()*1.0/Nobs,2)\n",
    "mzstack,mzprob,mzpoints=fz.plot_dpdfstack(zpdf,rdict.zgrid_out,zpoints[1],\n",
    "                                          magidx_test[:,2][model_sel],mageidx_test[:,2][model_sel],\n",
    "                                          mdict,[18,28,1.],'imag',\n",
    "                                          sel=sel&m2[:,2][model_sel],boxcar=10)\n",
    "title('$p_{t=3}<0.25$ objects ('+str(fsel)+')',y=1.02)\n",
    "\n",
    "figure(figsize=(14,8))\n",
    "sel=(model_Ptype[:,2]>0.25)&(model_Ptype[:,2]<0.75)\n",
    "fsel=round(sel.sum()*1.0/Nobs,2)\n",
    "mzstack,mzprob,mzpoints=fz.plot_dpdfstack(zpdf,rdict.zgrid_out,zpoints[1],\n",
    "                                          magidx_test[:,2][model_sel],mageidx_test[:,2][model_sel],\n",
    "                                          mdict,[18,28,1.],'imag',\n",
    "                                          sel=sel&m2[:,2][model_sel],boxcar=10)\n",
    "title('$0.25<p_{t=3}<0.75$ objects ('+str(fsel)+')',y=1.02)\n",
    "\n",
    "figure(figsize=(14,8))\n",
    "sel=(model_Ptype[:,2]>0.75)\n",
    "fsel=round(sel.sum()*1.0/Nobs,2)\n",
    "mzstack,mzprob,mzpoints=fz.plot_dpdfstack(zpdf,rdict.zgrid_out,zpoints[1],\n",
    "                                          magidx_test[:,2][model_sel],mageidx_test[:,2][model_sel],\n",
    "                                          mdict,[18,28,1.],'imag',\n",
    "                                          sel=sel&m2[:,2][model_sel],boxcar=10)\n",
    "title('$p_{t=3}>0.75$ objects ('+str(fsel)+')',y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these show that our spec-z/grism-z likelihood proportion per object is a rough proxy for magnitude, with predictions for fainter objects relying on larger fractions of COSMOS/3DHST photo-z's."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
