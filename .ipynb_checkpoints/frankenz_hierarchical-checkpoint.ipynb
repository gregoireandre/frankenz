{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian Modeling with FRANKEN-Z\n",
    "\n",
    "Now that we've demonstrated FRANKEN-Z works in the naive case, we will begin moving on to hierarchical modeling. Why would we want to do this? Two main motivations:\n",
    "- Hierarchical models allow the derivation of joint densities between the individual and population, which allows us to properly model **covariances**. This is crucial for understanding how the population can impact individual PDFs, and how the individual PDFs can impact the entire population (via their respective densities in flux space).\n",
    "- In addition, hierarchical models allow us to start including **selection effects** into our model. While the easiest case to understand involve simple flux boundaries, we'll leverage machine learning to try and incorporate more difficult selection functions.\n",
    "\n",
    "Adapting notation from Speagle et al. (2017), our baseline model is just:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{h},\\mathbf{f^h}|\\mathbf{g}) \\propto P(\\mathbf{f^h}) P(\\mathbf{h}|\\mathbf{f^h}) P(\\mathbf{g}|\\mathbf{h}) = P(\\mathbf{f^h})  \\prod_{(\\mathbf{h},\\mathbf{g})} P(h|\\mathbf{f^h}) P(g|h),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\mathbf{h}$ our training galaxies, $\\mathbf{g}$ our testing galaxies, and $\\mathbf{f^h}$ the parameters of our population prior within the training set.\n",
    "\n",
    "We can probe this distribution using a Gibbs sampler by alternating between sampling\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{f^h}|\\mathbf{h},\\mathbf{g})=P(\\mathbf{f^h}|\\mathbf{h}),\n",
    "\\end{equation}\n",
    "\n",
    "which is simply Dirichlet distributed, and \n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{h}|\\mathbf{f^h},\\mathbf{g})= \\prod_{(\\mathbf{h},\\mathbf{g})} P(h|\\mathbf{f^h}) P(g|h),\n",
    "\\end{equation}\n",
    "\n",
    "which is just the original likelihood modified by the new conditional population prior.\n",
    "\n",
    "To state things a bit more plainly, our hierarchical model has two parts: a population component and an individual component. To sample from the joint hierarchical distribution, all we have to do is sample from each of the conditional distributions: the population assuming we know the individual components, and then the individual components assuming we know the population. We continue this interative sampling until the distribution converges, at which point we are sampling from the true stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the case in Leistedt et al. (2016), we actually are working with a large batch of \"training data\" with arbitrary labels. As a result, the distribution we want to sample from is directly in the training data space with no imposed structure (i.e. which training object $h$ a target object $g$ is associated with). Once we have such the joint distribution between $\\mathbf{g}$ and $\\mathbf{h}$, we can then paint on any distribution we like using the (joint) mappings from $\\mathbf{h}$ to say $z$ (redshift) or $t$ (type), among others.\n",
    "\n",
    "The first thing to do is set our population prior $P(\\mathbf{f^h})$ and initialize our population parameter vector $\\mathbf{f}^{\\mathbf{h}}_0$. We will assume a uniform prior, so we'll just ignore that term for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh=zeros(Ntrain)\n",
    "for i in xrange(Ntest):\n",
    "    Nm=model_Nobj[i] # number of models\n",
    "    midx,ll=model_obj[i][:Nm],model_ll[i][:Nm] # model indices and corresponding log-likelihoods\n",
    "    llmin=ll.min() # minimum value (for scaling)\n",
    "    weights=exp(-0.5*(ll-llmin)) # weights\n",
    "    weights/=weights.sum() # normalize\n",
    "    fh[midx]+=weights # stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the great thing about FRANKEN-Z's sparse representation of the likelihood: not only are we able to quickly iterate over the testing objects for computations, but because we are able to actually store all the likelihoods **ahead of time** (which is impossible for brute-force approaches at this scale), we can re-use them in our later calculations without spending time recomputing the same quantities.\n",
    "\n",
    "There's also an additional bonus: because of the structure of our hierarchical model, we can automatically infer an individual object's PDF at any given iteration of our sampler using our pre-computed likelihoods and $\\mathbf{f^h}$. This means we don't have to store any of those interim values (which is memory intensive), but instead can generate them on demand as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything's all set up, we can easily generate a bunch of $\\mathbf{f^h}$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_samples=100\n",
    "fh_t=zeros((N_samples,len(fh)))\n",
    "\n",
    "# Gibbs sampler\n",
    "fh_t[0]=fh # initialize\n",
    "\n",
    "for i in xrange(N_samples-1):\n",
    "    sys.stdout.write(str(i)+' ')\n",
    "    fh_i=random.dirichlet(fh_t[i]+1) # draw population\n",
    "    for j in xrange(Ntest):\n",
    "        Nm=model_Nobj[j] # number of models\n",
    "        midx,ll=model_obj[j][:Nm],model_ll[j][:Nm] # model indices and corresponding log-likelihoods\n",
    "        llmin=ll.min() # minimum value (for scaling)\n",
    "        weights=exp(-0.5*(ll-llmin))*fh_i[midx] # (P(conditional)*likelihood)\n",
    "        weights/=weights.sum() # normalize\n",
    "        fh_t[i+1][midx]+=weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first examine how this impacts an individual object's PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j=28 # object index\n",
    "temp=array([fz.pdf_kde_dict(rdict.sig_dict,rdict.sig_width,\n",
    "                            lzidx_train_d[model_obj[j][:model_Nobj[j]]],lzeidx_train_d[model_obj[j][:model_Nobj[j]]],\n",
    "                            exp(-0.5*model_ll[j][:model_Nobj[j]])*fh_t[i][model_obj[j][:model_Nobj[j]]],\n",
    "                            rdict.grid,rdict.delta,rdict.Ngrid) for i in xrange(100)])\n",
    "temp/=temp.sum(axis=1)[:,None]\n",
    "\n",
    "figure(figsize=(14,8))\n",
    "cmap=matplotlib.cm.jet\n",
    "cmap.set_bad('white',1.)\n",
    "imshow(ma.array(temp,mask=temp<1e-3),aspect='auto',cmap=cmap)\n",
    "colorbar()\n",
    "xlabel('log(1+z) index')\n",
    "ylabel('Gibbs iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with standard Metropolis-Hastings Markov Chain Monte Carlo (MCMC) sampling, we first have to guarantee our sampler converges before we begin drawing values to derive a joint PDF. We can accomplish this using many sophisticated diagnostics, but because this is a simple test we just discard the first 10 values. It is then easy to map our new high-dimensional $\\mathbf{f^h}$ samples to redshift space using KDE.\n",
    "\n",
    "It is important to point out that we need as many samples are \"free parameters\" (flexible, since our gridding is arbitrary) to properly condition the covariance and/or precision matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_burnin=10 # burn-in trials\n",
    "z_pdf_draws=empty((N_samples-N_burnin,rdict.Nz_out)) # dn/dz draws\n",
    "\n",
    "for i in xrange(N_burnin,N_samples):\n",
    "    sys.stdout.write(str(i)+' ')\n",
    "    midx=choice(Ntrain,p=fh_t[i]/fh_t[i].sum(),size=Ntrain) # sample from f^h\n",
    "    z_pdf=fz.pdf_kde_dict(rdict.sig_dict,rdict.sig_width,lzidx_train_d[midx],lzeidx_train_d[midx],ones(Ntrain),\n",
    "                          rdict.grid,rdict.delta,rdict.Ngrid) # KDE dictionary PDF\n",
    "    z_pdf=z_pdf[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]/rdict.znorm\n",
    "    z_pdf=interp(rdict.zgrid_out,rdict.zgrid,z_pdf)\n",
    "    z_pdf/=z_pdf.sum()\n",
    "    z_pdf_draws[i-N_burnin]=z_pdf\n",
    "z_pdf_draws_mean=z_pdf_draws.mean(axis=0)\n",
    "z_pdf_draws_mean/=z_pdf_draws_mean.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,6))\n",
    "\n",
    "z_pdf=fz.pdf_kde_dict(rdict.sig_dict,rdict.sig_width,lzidx_test_d,lzeidx_test_d,ones(Ntest),\n",
    "                      rdict.grid,rdict.delta,rdict.Ngrid) # KDE dictionary PDF\n",
    "z_pdf=z_pdf[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]/rdict.znorm\n",
    "z_pdf=interp(rdict.zgrid_out,rdict.zgrid,z_pdf)\n",
    "z_pdf/=z_pdf.sum()\n",
    "plot(rdict.zgrid_out,z_pdf,lw=5,color='blue',label='Underlying')\n",
    "\n",
    "plot(rdict.zgrid_out,z_pdf_draws_mean,lw=5,color='black',alpha=0.5,label='Mean')\n",
    "plot(rdict.zgrid_out,z_pdf_draws[0],lw=0.5,color='red',alpha=0.3,label='Sample')\n",
    "[plot(rdict.zgrid_out,z_pdf_draws[i+1],lw=0.5,color='red',alpha=0.3) for i in xrange(N_samples-N_burnin-1)]\n",
    "plot(rdict.zgrid_out,z_pdf_draws_mean,lw=5,color='black',alpha=0.5,label='Mean')\n",
    "\n",
    "xlim([rdict.zgrid_out.min(),rdict.zgrid_out.max()])\n",
    "tight_layout()\n",
    "legend(fontsize=20)\n",
    "xlabel('Redshift')\n",
    "yticks([])\n",
    "ylabel('Normalized Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(14,10))\n",
    "Q_pois,Q_ad,Q_err=fz.plot_nz(z_pdf*Ntest,z_pdf_draws_mean*Ntest,rdict.zgrid_out,rdict.dz_out,out_nz_draws=z_pdf_draws*Ntest,\n",
    "                             sample_names=['Underlying','Hierarchical Bayes'],colors=['black','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that we get broadly similar error behavior between the naive hierarchical modeling errors computed from the samples (ignoring covariances among redshift bins) at $z<3$, especially when the number density is high. However, at lower number densities, we see that our hierarhical modeling errors systematically underestimate the true amount of possible variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating selection effects\n",
    "\n",
    "We now add a little bit of complexity to our model by adding in the impact of selection effects on the testing objects $\\mathbf{g}$ using a new selection flag $\\boldsymbol{\\gamma}$.\n",
    "\n",
    "Our baseline model then becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{h},\\mathbf{f^h}|\\mathbf{g},\\boldsymbol{\\gamma}) \\propto P(\\mathbf{f^h}|\\boldsymbol{\\gamma}) P(\\mathbf{h}|\\mathbf{f^h},\\boldsymbol{\\gamma}) P(\\mathbf{g}|\\mathbf{h},\\boldsymbol{\\gamma}) = P(\\mathbf{f^h})  \\prod_{(\\mathbf{h},\\mathbf{g})} P(h|\\mathbf{f^h}) P(g|h,\\gamma).\n",
    "\\end{equation}\n",
    "\n",
    "The only change relative to our original model is the term $P(g|h,\\gamma)$, which can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(g|h,\\gamma)=\\frac{P(g|h)P(\\gamma|g)}{P(\\gamma|h)}=\\frac{P(g|h)P(\\gamma|g)}{\\int P(g|h)P(\\gamma|g) dg},\n",
    "\\end{equation}\n",
    "\n",
    "where the integral in the denominator is taken over all possible flux realizations of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to outline an explicit example of how this works for a simple selection function: a multi-dimensional flux cut along some flux vector $\\mathbf{F}_{cut}$. In the scale-dependent likelihood case where the flux density for object $i$ with mean $\\mathbf{\\hat{F}}_i$ and covariance $\\mathbf{\\hat{C}}_i$ is normally distributed with PDF $\\mathcal{N}(\\mathbf{F}|\\mathbf{\\hat{F}}_i,\\mathbf{\\hat{C}}_i)$, then \n",
    "\n",
    "\\begin{equation}\n",
    "P(g|h) = \\frac{1}{C} \\int \\mathcal{N}(\\mathbf{F}|\\mathbf{\\hat{F}}_g,\\mathbf{\\hat{C}}_g)\\mathcal{N}(\\mathbf{F}|\\mathbf{\\hat{F}}_h,\\mathbf{\\hat{C}}_h) d\\mathbf{F} = \\frac{S_{hg}}{\\sum_h S_{hg}},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S_{hg} = \\mathcal{N}(\\mathbf{\\hat{F}}_h|\\mathbf{\\hat{F}}_{g},\\mathbf{\\hat{C}}_{h}+\\mathbf{\\hat{C}}_{g}).\n",
    "\\end{equation}\n",
    "\n",
    "Our selection function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\gamma|g)=\\prod_i H(\\hat{F}_{g,i}-(F_{cut})_i)=H(\\mathbf{\\hat{F}}_g-\\mathbf{F}_{cut}),\n",
    "\\end{equation}\n",
    "\n",
    "where $H$ is the Heavyside function, which evaluates to 1 for all $\\mathbf{\\hat{F}}_g>\\mathbf{F}_{cut}$ elementwise and 0 otherwise.\n",
    "\n",
    "The denominator is now the convolution of our original likelihood with the selection function, which gives:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\gamma|h) = \\int P(g|h)P(\\gamma|g) dg = \\frac{1}{C} \\int \\mathcal{N}(\\mathbf{\\hat{F}}_g|\\mathbf{\\hat{F}}_{h},\\mathbf{\\hat{C}}_{h}+\\mathbf{\\hat{C}}_{g}) H(\\mathbf{\\hat{F}}_g-\\mathbf{F}_{cut}) d\\mathbf{\\hat{F}}_g = \\frac{S^\\prime_{hg}(\\mathbf{F}_{cut})}{\\sum_h S^\\prime_{hg}(\\mathbf{F}_{cut})},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S^\\prime_{hg}(\\mathbf{F}_{cut}) \\equiv \\int_{\\mathbf{F}_{cut}}^{+\\infty} \\mathcal{N}(\\mathbf{\\hat{F}}_g|\\mathbf{\\hat{F}}_{h},\\mathbf{\\hat{C}}_{h}+\\mathbf{\\hat{C}}_{g}) d\\mathbf{\\hat{F}}_g.\n",
    "\\end{equation} \n",
    "\n",
    "This gives:\n",
    "\n",
    "\\begin{equation}\n",
    "P(g|h,\\gamma)=\\frac{P(g|h)P(\\gamma|g)}{P(\\gamma|h)} = \\frac{\\sum_h S^\\prime_{hg}(\\mathbf{F}_{cut})}{\\sum_h S_{hg}} \\frac{S_{hg}}{S^\\prime_{hg}(\\mathbf{F}_{cut})},\n",
    "\\end{equation}\n",
    "\n",
    "since by definition $H(\\mathbf{\\hat{F}}_g-\\mathbf{F}_{cut})=1$ for all all objects included in the sample. In other words, we **boost** the relative contribution from objects by an amount inversely proportional to the **fraction of the original likelihood contained within the selection boundary**, which ranges from $1 \\leq 1/S^\\prime_{hg}(\\mathbf{F}_{cut}) \\leq 2^d$ where $d$ is the number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUFF about selection flux marginalization machine learning. We know what the MVG is for P(g|h) w/ associated normalization from the likelihoods and such, so that's easy. The second term might have some complicated form, necessitating numerical integration. If we Monte Carlo some fluxes, we should be able to evaluate it. Let's try the simple selection first and then scale up to something harder w/ ML.\n",
    "\n",
    "To state things a bit more plainly, STUFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1,t2=phot_train_d[1][2],sqrt(var_train_d[1][2])\n",
    "samples=normal(t1,t2,size=(10000))\n",
    "qx=fz.gaussian(t1,t2**2,samples)\n",
    "fx=fz.gaussian(t1,(t2*0.7)**2,samples)\n",
    "sum(fx/qx)/len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(samples,qx,'.')\n",
    "plot(samples,fx,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcut=phot_train_d[2]\n",
    "fmu=phot_train_d[2]\n",
    "fstd=sqrt(var_train_d[2])\n",
    "t2=(fcut-fmu)/(sqrt(2)*fstd)\n",
    "print t2\n",
    "print 1./prod(1-0.5*erfc(-t2)),2**5 # Diag-Cov Multivariate Gaussian Integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
