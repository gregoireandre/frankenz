########## MODULES ##########

# general environment
import numpy as np
import matplotlib
import scipy
from numpy import *
from numpy.random import *
from matplotlib import *
from matplotlib.pyplot import *
from scipy import *

# general functions
import pandas # uniqueness checks
import sys # print statements
from astropy.io import fits # I/O on fits
from sklearn.externals import joblib # I/O on ML models
import os # used to check for files
from scipy import interpolate # interpolation

# pre-processing and cross-validation
from sklearn import cross_validation
from sklearn.cross_validation import train_test_split
from sklearn import decomposition
from sklearn.preprocessing import StandardScaler

# machine learning
from sklearn import tree # decision trees
from sklearn import base # additional methods

# statistics
from scipy import stats
from scipy import random

# additional memory management
import gc

# confidence intervals
SIG1=68.2689492/100.
SIG2=95.4499736/100.
SIG3=99.7300204/100.


# pre-defined constants
l2pi=log(2*pi)


########## PLOTTING DEFAULTS ##########


# declaring plotting stuff
from matplotlib.font_manager import FontProperties
from matplotlib import gridspec
rcParams.update({'xtick.major.pad': '7.0'})
rcParams.update({'xtick.major.size': '7.5'})
rcParams.update({'xtick.major.width': '1.5'})
rcParams.update({'xtick.minor.pad': '7.0'})
rcParams.update({'xtick.minor.size': '3.5'})
rcParams.update({'xtick.minor.width': '1.0'})
rcParams.update({'ytick.major.pad': '7.0'})
rcParams.update({'ytick.major.size': '7.5'})
rcParams.update({'ytick.major.width': '1.5'})
rcParams.update({'ytick.minor.pad': '7.0'})
rcParams.update({'ytick.minor.size': '3.5'})
rcParams.update({'ytick.minor.width': '1.0'})
rcParams.update({'xtick.color': 'k'})
rcParams.update({'ytick.color': 'k'})
rcParams.update({'font.size': 30})










########## MODEL FITTING ##########

# Profile: Scales as O(Nmodel*Nfilter), with a fixed baseline of ~25us and scaling of ~9.61us/filter and ~0.11us/model (~135us for 1e3 models in 5 bands).
def chi2(data, data_var, data_lnorm, models):
    """
    Compute chi2 value for a SINGLE OBJECT using a FIXED SET of models.

    Keyword arguments:
    data -- input fluxes
    data_var -- input variances
    data_lnorm -- ln(normalization) of multivariate Gaussian PDF
    models -- collection of comparison models

    Outputs:
    norm_vals, chi2_vals, shape_vals, chi2_vals_mod
    norm_vals -- associated maximum-likelihood scalefactors
    chi2_vals -- fitted chi2 values (OPTIMIZING over scalefactor)
    shape_vals -- fitted 'shapefactor' values (i.e. quadratic term) for chi2
    chi2_vals_mod -- modified chi2 values (MARGINALIZING over scalefactor)
    """
    
    # derive scalefactors between data and models
    t1=models*data[None,:]/data_var[None,:] # interaction term
    inter_vals=sum(t1,axis=1)

    t2=models*models/data_var[None,:] # model-dependent term
    shape_vals=sum(t2,axis=1) # governs 'steepness' of chi2 quadratic function
    
    norm_vals=inter_vals/shape_vals # maximum-likelihood scalefactor

    # compute chi2
    smodels=norm_vals[:,None]*models # scale models
    resid=data-smodels # compute residuals
    chi2_vals=sum(resid*resid/data_var[None,:],axis=1) # compute chi2

    # compute
    mod_norm=shape_vals*norm_vals*norm_vals
    chi2_vals_mod=chi2_vals+data_lnorm-log(mod_norm)-l2pi
    
    return norm_vals, chi2_vals, shape_vals, chi2_vals_mod









########## QUANTIFYING PERFORMANCE #############


def compute_score(y, pred, eta=0.15, weights=None):
    """
    Compute associated quality scores between two sets of matched redshift predictions.

    Keyword arguments:
    y -- input set of targets
    pred -- matching set of predictions
    eta -- threshold used for computing catastrophic outliers (|pred-y|/1+y > eta)

    Outputs:
    [mean scores, median scores, r2, pearson-r, f_cat]
    mean scores -- mean(deltaz), std(deltaz), mean(deltaz_p), std(deltaz_p), where deltaz_p=deltaz/(1+y).
    median scores -- median(deltaz), MAD(deltaz; 1-sigma), median(deltaz_p), MAD(deltaz_p; 1-sigma)
    r2 -- R2 correlation coefficient
    pearson-r -- Pearson R coefficient, p-value
    f_cat -- catastrophic outlier fraction
    """

    if weights is not None:
        weights=weights
    else:
        weights=ones(len(y))

    Nobj=sum(weights)
    Ny=len(y)
    
    sig1=int(SIG1*Nobj) # defining 1-sigma boundary
    deltaz,deltaz_p=pred-y,(pred-y)/(1+y) # defining offsets and (1+z)-normalized offsets

    # compute mean stats
    mean_dz,mean_dzp=average(deltaz,weights=weights),average(deltaz_p,weights=weights)
    std_dz,std_dzp=sqrt(average((deltaz-mean_dz)**2,weights=weights)),sqrt(average((deltaz_p-mean_dzp)**2,weights=weights))
    mean_stat=[mean_dz,std_dz,mean_dzp,std_dzp] # mean scores
    
    # compute median stats
    s1,s2=argsort(deltaz),argsort(deltaz_p)
    cdf1,cdf2=cumsum(weights[s1]),cumsum(weights[s2])
    med_dz,med_dzp=deltaz[s1][argmin(abs(cdf1-0.5*Nobj))],deltaz[s2][argmin(abs(cdf2-0.5*Nobj))]
    mad1,mad2=abs(deltaz-med_dz),abs(deltaz_p-med_dzp)
    s1,s2=argsort(mad1),argsort(mad2)
    cdf1,cdf2=cumsum(weights[s1]),cumsum(weights[s2])
    mad_dz,mad_dzp=mad1[s1][argmin(abs(cdf1-SIG1*Nobj))],mad2[s2][argmin(abs(cdf2-SIG1*Nobj))]
    med_stat=[med_dz,mad_dz,med_dzp,mad_dzp] # median scores

    # compute R2 stats
    r2_stat=1-sum(weights*deltaz**2)/sum(weights*(y-average(y,weights=weights))**2) # R2

    # compute pearsonr stats
    temp=[]
    wtemp=weights/sum(weights)
    for i in xrange(10):
        idx=choice(Ny,p=wtemp,size=int(Nobj))
        temp.append(stats.pearsonr(y[idx],pred[idx]))
    pearsonr_coeff=average(temp,axis=0) # Pearson R
    
    # bundle stats
    eta_cat=sum(weights[abs(deltaz_p)>0.15])*1.0/Nobj # f_cat
    
    return [mean_stat, med_stat, r2_stat, pearsonr_coeff, eta_cat]


def compute_density_score(dist_base, dist_test):
    """
    Compute quality scores for two sets of matched densities.

    Keyword arguments:
    dist_base -- the baseline distribution to be compared to
    dist_test -- the new distribution to be tested

    Outputs:
    [N_poisson statistic, arr_poisson, sel_poisson], KS_statistic
    N_poisson -- the total Poisson deviation between the two datasets
    arr_poisson -- the cumulative sum used to compute N_poisson
    sel_poisson -- all non-zero terms used in the sum
    KS_statistic -- the Kolmogorov-Smirnov 2-sample test statistics
    """

    a,b=dist_base.flatten(),dist_test.flatten() # flatten arrays to 1-D
    a_sub=a>0 # find all positive terms
    arr_poisson=sqrt(cumsum(((a-b)*(a-b)/a)[a_sub])/sum(a_sub)) # compute normalized Poisson fluctuation
    arr_p_poisson=stats.distributions.poisson.cdf(a-abs(a-b),a)+(1-stats.distributions.poisson.cdf(a+abs(a-b),a))
    N_poisson=arr_poisson[-1] # select final value
    N_ks=stats.ks_2samp(a,b) # compute KS 2-sample test score

    return [N_poisson, arr_poisson, arr_p_poisson, a_sub], N_ks









########### PDF FUNCTIONS ###############


def pdfs_resample(target_grid, target_pdfs, new_grid, deg_spline='linear'):
    """
    Resample input PDFs from a given grid onto a new grid.

    Keyword arguments:
    target_grid -- original grid
    target_pdfs -- original collection of PDFs
    new_grid -- new grid
    deg_spline -- order of spline fit

    Outputs:
    resampled PDFs
    """
    
    sys.stderr.write("Resampling PDFs...")
    
    Nobj,Npoints=len(target_pdfs),len(new_grid) # grab size of inputs
    new_pdfs=zeros((Nobj,Npoints)) # create new array
    for i in xrange(Nobj):
        if i%5000==0: sys.stderr.write(str(i)+" ")
        new_pdfs[i]=interpolate.interp1d(target_grid,target_pdfs[i],kind=deg_spline)(new_grid) # create and evaluate spline
        new_pdfs[i]/=trapz(new_pdfs[i],new_grid) # re-normalize
        
    sys.stderr.write("done!\n")

    return new_pdfs



def pdfs_summary_statistics(target_grid,target_pdfs,res,conf_width=0.03,deg_spline='linear'):
    """
    Compute a range of summary statistics from the input PDFs.

    Keyword arguments:
    target_grid -- input grid
    target_pdfs -- input collection of PDFs
    res -- resolution of resampled grid (used to compute confidence intervals)
    conf_width -- redshift range used to establish the 'zConf' flag (see Carrasco Kind & Brunner 2013) 
    deg_spline -- order of spline fit 

    Outputs:
    pdf_mean, pdf_med, pdf_mode, pdf_l68, pdf_h68, pdf_l95, pdf_h95, pdf_std, pdf_conf
    pdf_mean -- mean (first moment)
    pdf_med -- median (50th percentile)
    pdf_mode -- peak (mode)
    pdf_l68 -- lower 68th percentile of CDF
    pdf_h68 -- higher 68th percentile of CDP
    pdf_l95 -- lower 95th percentile of CDF
    pdf_h95 -- higher 95th percentile of CDF
    pdf_std -- standard deviation (sqrt of normalized second moment)
    pdf_conf -- zConf flag (see Carrasco Kind & Brunner 2013)
    """

    # initialize variables
    Ntest=len(target_pdfs) # number of input pdfs
    grid_fine=arange(target_grid[0],target_grid[-1]+res/2.,res) # resampled grid
    
    pdf_mean=zeros(Ntest,dtype='float32') # mean (first moment)
    pdf_std=zeros(Ntest,dtype='float32') # standard deviation (sqrt of normalized second moment)
    pdf_mode=zeros(Ntest,dtype='float32') # peak (mode)
    pdf_med=zeros(Ntest,dtype='float32') # median
    pdf_l95=zeros(Ntest,dtype='float32') # lower 95% confidence interval
    pdf_l68=zeros(Ntest,dtype='float32') # lower 68% confidence interval
    pdf_h68=zeros(Ntest,dtype='float32') # upper 68% confidence interval
    pdf_h95=zeros(Ntest,dtype='float32') # upper 95% confidence interval
    pdf_conf=zeros(Ntest,dtype='float32') # zConf flag


    # confidence intervals
    i1=0.68 # interval 1
    i2=0.95 # interval 2

    m=0.5 # median
    l2=m-i2/2 # lower 2
    l1=m-i1/2 # lower 1
    u1=m+i1/2 # upper 1
    u2=m+i2/2 # upper 2

    
    sys.stderr.write("Computing PDF quantities...")
    
    for i in xrange(Ntest):
        if i%5000==0: sys.stderr.write(str(i)+" ")
        
        # mean quantities
        pdf_mean[i]=trapz(target_pdfs[i]*target_grid,target_grid)
        pdf_std[i]=(trapz(target_pdfs[i]*(target_grid-pdf_mean[i])**2,target_grid))**(1./2.)
        
        # mode quantities
        pdf_mode[i]=target_grid[argmax(target_pdfs[i])]
        
        # cumulative distribution function
        cdf=cumsum(target_pdfs[i]) # original CDF
        cdf/=cdf[-1]
        cdf_function=interpolate.interp1d(target_grid,cdf)
        cpdf=cdf_function(grid_fine) # resampled CDF

        # median quantities
        pdf_med[i]=grid_fine[argmin(abs(cpdf-m))]
        pdf_h68[i]=grid_fine[argmin(abs(cpdf-u1))]
        pdf_l68[i]=grid_fine[argmin(abs(cpdf-l1))]
        pdf_h95[i]=grid_fine[argmin(abs(cpdf-u2))]
        pdf_l95[i]=grid_fine[argmin(abs(cpdf-l2))]
        
        # confidence flag
        conf_range=conf_width*(1+pdf_med[i])
        pdf_conf[i]=cdf_function(min(pdf_med[i]+conf_range,grid_fine[-1]))-cdf_function(max(pdf_med[i]-conf_range,grid_fine[0]))
        
    sys.stderr.write("done!\n")

    return pdf_mean, pdf_med, pdf_mode, pdf_l68, pdf_h68, pdf_l95, pdf_h95, pdf_std, pdf_conf












############# KERNAL DENSITY ESTIMATION ################


def gaussian(mu, var, x):
    """
    Compute (normalized) Gaussian kernal.

    Keyword arguments:
    mu -- mean (center)
    var -- variance (width)
    x -- input grid for computation

    Outputs:
    Gaussian kernel evaluated over x
    """
    
    dif=(x-mu) # difference
    norm=1/sqrt(2*pi*var) # normalization
    
    return norm*exp(-0.5*dif*dif/var)



# Compute smoothed PDF using kernel density estimation (KDE) from a set of WEIGHTED predictions
def pdf_kde_wt(y, y_var, y_wt, Npred_counter, pdf_grid, delta_grid, Ngrid, wt_thresh=1e-3):
    """
    Compute smoothed PDF from point estimates using kernel density estimation (KDE) from a set of WEIGHTED predictions.

    Keyword arguments:
    y -- observed data
    y_var -- variance of observed data
    y_wt -- weight of observed data
    Npred_counter -- counter to step through data
    pdf_grid -- underlying grid used to compute the probability distribution function (PDF)
    delta_grid -- spacing of the grid
    Ngrid -- number of elements in the grid
    wt_thresh -- threshold used (relative to maximum weight) to include weighted observations in the computation 

    Outputs:
    pdf -- probability distribution function (PDF) evaluated on 'pdf_grid'
    """

    # limit kernel to +/-5 sigma
    centers=((y-pdf_grid[0])/delta_grid).astype(int)
    variances=delta_grid*delta_grid+y_var
    offsets=(5*sqrt(variances)/delta_grid).astype(int)
    uppers,lowers=centers+offsets,centers-offsets
    uppers[uppers>Ngrid],lowers[lowers<0]=Ngrid,0

    # limit analysis to observations with statistically relevant weight
    threshold=wt_thresh*max(y_wt)
    sel_arr=Npred_counter[y_wt>threshold].astype('int') # select all observations that pass the cutoff threshold

    # initialize PDF
    pdf=zeros(Ngrid)

    # compute PDF array
    for i in sel_arr:
        pdf[lowers[i]:uppers[i]]+=y_wt[i]*gaussian(y[i],variances[i],pdf_grid[lowers[i]:uppers[i]]) # stack (weighted) Gaussian kernels on array slices
    
    return pdf



# Compute smoothed PDF using kernel density estimation (KDE) from a set of WEIGHTED predictions
def pdf_kde_wt_dict(ydict, ywidth, y_pos, y_idx, y_wt, pdf_grid, delta_grid, Ngrid, wt_thresh=1e-3):
    """
    Compute smoothed PDF from point estimates using kernel density estimation (KDE) from a set of WEIGHTED predictions and a PRE-COMPUTED DICTIONARY.

    Keyword arguments:
    ydict -- dictionary of kernels
    ywidth -- associated widths of kernels
    y_pos -- discretized position of observed data
    y_idx -- corresponding index of kernel drawn from dictionary
    y_wt -- weight of observed data
    pdf_grid -- underlying grid used to compute the probability distribution function (PDF)
    delta_grid -- spacing of the grid
    Ngrid -- number of elements in the grid
    wt_thresh -- threshold used (relative to maximum weight) to include weighted observations in the computation

    Outputs:
    pdf -- probability distribution function (PDF) evaluated on 'pdf_grid'
    """

    # initialize PDF
    pdf=zeros(Ngrid) 

    # limit analysis to observations with statistically relevant weight
    sel_arr=y_wt>wt_thresh*max(y_wt)

    # stack kernels
    for i in arange(len(y_idx))[sel_arr]: # run over selected observations
        idx,yp=y_idx[i],y_pos[i] # dictionary element, kernel center
        yw=ywidth[idx] # kernel width
        pdf[yp-yw:yp+yw+1]+=y_wt[i]*ydict[idx] # stack weighted kernel from dictionary on array slice
    
    return pdf


















############# SELF-ORGANIZING MAP #################

class som_map():
    """ 
    Generic SOM instance.
    """

    def __init__(self):
        """
        Initialize instance.
        """

class SOM():
    """
    The Self-Organizing Map (SOM) class.

    Functions:
    train -- trains the SOM
    map_bmu -- maps objects onto the SOM using BMU matching (DISCRETE mapping)
    map_bmu_subset -- recomputes number densities based on input selection array
    pdf_bmu -- generate SOM-based PDFs based on BMU mappings from func::map_bmu
    map_chi2 -- maps objects onto the SOM using chi2-based PDFs (CONTINUOUS mapping)
    map_chi2_subset -- recomputes number densities/likelihoods based on input selection array
    pdf_chi2 -- generate SOM-based PDFs based on chi2 mappings from func::map_chi2 
    """

    def __init__(self, Nfilt, sparams=None, dimensions=[50,50], A=[0.5,0.1], H=[35.0,1.0], Niter=100000):
        """
        Keyword arguments:
        Nfilt -- number of filters (i.e. dimensionality of target space)
        sparams -- SOM parameters (from class::ReadParams)
        dimensions -- dimensionality/size of the SOM
        a -- the learning rate (initial, final)
        h -- the neighborhood function (initial, final)
        Niter -- number of training iterations
        """

        
        # extract parameters
        if sparams is not None:
            dimensions=[int(sparams['SOM_DIM_'+str(i+1)]) for i in xrange(int(sparams['DIMENSIONS']))]
            A=[sparams['LEARNING_RATE_INITIAL'],sparams['LEARNING_RATE_FINAL']]
            H=[sparams['NEIGHBORHOOD_INITIAL'],sparams['NEIGHBORHOOD_FINAL']]
            Niter=int(sparams['N_ITER'])

        self.dimensions=copy(dimensions)
        self.learning_rate=copy(A)
        self.neighborhood=copy(H)
        self.Niter=Niter

        # record basic SOM properties
        self.Nfilt=Nfilt
        self.Ndim=len(self.dimensions)
        self.Ncell=product(self.dimensions)
        
        # initialize SOM
        self.cellmodel=zeros((self.Ncell,self.Nfilt),dtype='float32')
        self.position=zeros((self.Ncell,self.Ndim),dtype='int')

        for i in xrange(self.Ncell):
            temp=i

            # assign N-D position
            for j in xrange(self.Ndim): 
                pos_temp=1 # flat 1-D index
                for k in xrange(j+1,self.Ndim):
                    pos_temp*=self.dimensions[k] # iterative step size
                self.position[i][j]=temp/pos_temp # associated block
                temp-=self.position[i][j]*pos_temp # decrement 1-D index

            self.cellmodel[i]=uniform(0,1,self.Nfilt) # randomized cell model


    def initialize_prior(self, color_asinh, lambda_eff, color_pivots, prior_bounds):
        """
        Assing SOM cell priors based on data inputs.

        Keyword arguments: 
        color_asinh -- input colors
        lambda_eff -- filter wavelengths
        color_pivots -- pivot colors
        prior_bounds -- confidence interval of color to span
        """

        N=len(color_asinh)
        seed1=linspace(
            sort(color_asinh[:,color_pivots[0]])[int(prior_bounds[0]*N)],
            sort(color_asinh[:,color_pivots[0]])[int(prior_bounds[1]*N)],
            self.dimensions[0]) # seed for SOM dimension 1
        seed2=linspace(
            sort(color_asinh[:,color_pivots[1]])[int(prior_bounds[0]*N)],
            sort(color_asinh[:,color_pivots[1]])[int(prior_bounds[1]*N)],
            self.dimensions[1]) # seed for SOM dimension 1

        # populate SOM prior
        som_seed=ones((self.dimensions[0],self.dimensions[1],self.Nfilt),dtype='float32')
        for count1 in xrange(self.dimensions[0]):
            for count2 in xrange(self.dimensions[1]):
                for i in xrange(self.Nfilt-1):
                    if i<color_pivots[1]:
                        t1=seed1[count1]/(lambda_eff[1]-lambda_eff[0]) # color dy/dlambda
                        t2=lambda_eff[i+1]-lambda_eff[i] # dlambda
                        som_seed[count1][count2][i+1]=som_seed[count1][count2][i]*10**(0.4*t1*t2)
                    else:
                        t1=seed2[count2]/(lambda_eff[color_pivots[1]+1]-lambda_eff[color_pivots[1]]) # color dy/dlambda
                        t2=lambda_eff[i+1]-lambda_eff[i] # dlambda
                        som_seed[count1][count2][i+1]=som_seed[count1][count2][i]*10**(0.4*t1*t2)
                
        # assign SOM cell priors            
        for i in xrange(self.Ncell):
            pos_temp=tuple(self.position[i])
            self.cellmodel[i]=som_seed[pos_temp]


    def train(self, X, X_var, X_lnorm, norm_band, sigma_trunc=5.0):
        """
        Train the Self-Organizing Map (SOM) on the input data. 

        Keyword arguments:
        X -- collection of features (fluxes)
        X_var -- associated collection of feature variances (flux errors)
        X_lnorm -- ln(normalization) of multivariate Gaussian PDF
        norm_band -- band used to normalize cell models
        sigma_trunc -- number of standard deviations to explore before truncating operations
        """

        # initializing variables
        Nobj=len(X) # training sample size
        som_position=array(self.position,dtype='float32') # convert to floats (for distance computations)
        rand_idx=random.choice(xrange(Nobj),self.Niter,replace=True) # pre-generated random numbers        
        flux,flux_var,flux_lnorm=X[rand_idx],X_var[rand_idx],X_lnorm[rand_idx] # pre-selected random objects
        a_t=self.learning_rate[0]-(self.learning_rate[0]-self.learning_rate[1])/self.Niter*arange(self.Niter) # learning rate
        sigma_t=self.neighborhood[0]-(self.neighborhood[0]-self.neighborhood[1])/self.Niter*arange(self.Niter) # neighborhood
        var_t=sigma_t**2
        var_trunc=sigma_trunc**2

        for t in xrange(self.Niter):
            if t%10000==0: sys.stderr.write(str(t)+' ')

            # compare random object to SOM cells
            cellnorm,cellchi2,cellshape,cellchi2mod=chi2(flux[t],flux_var[t],flux_lnorm[t],self.cellmodel) # compute chi2 to cell models
            best_idx=argmin(cellchi2mod) # grab best-matching cell (unit) index (BMU)

            # select cells to update
            dif=som_position[best_idx]-som_position # difference
            sq_dist=sum(dif*dif,axis=1) # squared distance
            sel_arr=sq_dist<var_trunc*var_t[t] # select only cells within 5-sigma of BMU
        
            # update cells
            cellmodel_renorm=cellnorm[sel_arr][:,None]*self.cellmodel[sel_arr] # renormalizing cell models
            cell_dif=flux[t]-cellmodel_renorm # computing flux residuals
            cell_update=a_t[t]*exp(-0.5*sq_dist[sel_arr]/var_t[t]) # calculating learning update
            self.cellmodel[sel_arr]=cellmodel_renorm+cell_update[:,None]*cell_dif # update cells

        # normalize cell models
        self.cellmodel/=self.cellmodel[:,int(norm_band)][:,None]

        # compute distance to nearest neighbor
        self.cell_dNN=array([min(sum((self.cellmodel[i]-append(self.cellmodel[:i],self.cellmodel[i+1:],axis=0))**2,axis=1)) for i in xrange(self.Ncell)])


    def map_bmu(self, X, X_var, X_lnorm, redchi2_thresh=5.0):
        """
        Map objects onto the SOM based on the best-matching unit (BMU). This generates a DISCRETE mapping between each object and a particular cell.

        Keyword arguments:
        X -- collection of features (fluxes)
        X_var -- associated collection of feature variances (flux errors)
        X_lnorm -- ln(normalization) of multivariate Gaussian PDF
        redchi2_thresh -- reduced-chi2 threshold to differentiate samples

        Outputs:
        som_map class with the following initialized parameters:
        bmu_pos -- SOM BMU indices
        bmu_norm -- associated normalizations
        bmu_steep -- associated chi2 quadratic terms
        bmu_chi2 -- associated chi2 values (OPTIMIZED over normalization)
        bmu_chi2mod -- associated chi2 values (MARGINALIZED over normalization)
        bmu_flag -- flag based on reduced-chi2 threshold
        cell_Nobj -- number of objects in a given cell
        cell_Nobj_clean -- number of objects in a given cell after applying bmu_flag
        """

        Nobj=len(X)
        dof=self.Nfilt-1 # degrees of freedom

        # initialize best-matching unit (BMU) variables
        bmu_pos=zeros(Nobj,dtype='int')
        bmu_norm=zeros(Nobj,dtype='float32')
        bmu_shape=zeros(Nobj,dtype='float32')
        bmu_chi2=zeros(Nobj,dtype='float32')
        bmu_chi2mod=zeros(Nobj,dtype='float32')

        for i in xrange(Nobj): # for each object
            if i%10000==0: sys.stderr.write(str(i)+' ')
            norm_vals,chi2_vals,shape_vals,chi2mod_vals=chi2(X[i],X_var[i],X_lnorm[i],self.cellmodel) # fit SOM cell models
            idx=argmin(chi2mod_vals) # select BMU
            bmu_pos[i]=idx # assign index
            bmu_norm[i],bmu_shape[i],bmu_chi2[i],bmu_chi2mod[i]=norm_vals[idx],shape_vals[idx],chi2_vals[idx],chi2mod_vals[idx] # assign chi2-based values

        bmu_flag=(bmu_chi2/dof<redchi2_thresh) # reduced-chi2 flag

        # initialize som_map object
        smap=som_map()

        smap.bmu_pos=bmu_pos
        smap.bmu_norm=bmu_norm
        smap.bmu_steep=bmu_steep
        smap.bmu_chi2=bmu_chi2
        smap.bmu_chi2mod=bmu_chi2mod
        smap.bmu_flag=bmu_flag

        smap.cell_Nobj=array([sum(bmu_pos==i) for i in xrange(self.Ncell)],dtype='int')
        smap.cell_Nobj_clean=array([sum((bmu_pos==i)&bmu_flag) for i in xrange(self.Ncell)],dtype='int')
        
        return smap


    def map_bmu_subset(self, smap, sel):
        """
        Re-compute number densities given a new selection array.

        Keyword arguments:
        smap -- som_map instance (see func::SOM.map_bmu)
        sel -- boolean selection array
        """

        bmu_Nobj_new=array([sum((smap.bmu_pos==i)&sel) for i in xrange(self.Ncell)],dtype='int')

        return bmu_Nobj_new


    def pdf_bmu(self, bmu_pos, y_pos, y_idx, ydict, ywidth, pdf_grid, delta_grid, Ngrid, sel=None):
        """
        Compute PDFs for objects given their corresponding SOM BMUs.

        Keyword arguments:
        bmu_pos -- index of BMU
        y_pos -- discretized index of target
        y_idx -- matching kernel (width) index
        ydict -- dictionary of Gaussian kernels
        ywidth -- width of kernels
        pdf_grid -- underlying grid used for fitting
        delta_grid -- spacing of grid
        Ngrid -- number of elements in grid
        sel -- boolean selection array of input objects

        Outputs:
        som_pdf -- stacked PDF computed within each som cell
        """

        # initialize variables
        som_pdf=zeros((self.Ncell,Ngrid),dtype='float32')

        # select objects
        if sel is not None:
            bmu_pos=bmu_pos[sel]
            y_pos=y_pos[sel]
            y_idx=y_idx[sel]

        # compute PDF(cell)
        for i in xrange(self.Ncell):
            if i%100==0: sys.stderr.write(str(i)+' ')
            cell_sel=(bmu_pos==i) # select objects sorted into BMU
            Nobj=sum(cell_sel)
            if som_Nobj[i]>0:
                som_pdf[i]=pdf_kde_wt_dict(ydict,ywidth,y_pos[cell_sel],y_idx[cell_sel],ones(Nobj),pdf_grid,delta_grid,Ngrid)/Nobj

        return som_pdf
        

    def map_chi2(self, X, X_var, X_lnorm, wt_thresh=1e-3, redchi2_thresh=5.0):
        """
        Map objects onto the SOM based on their likelihood distribution across all cells. This generates a CONTINUOUS mapping between each object and a particular cell.

        Keyword arguments:
        X -- collection of features (fluxes)
        X_var -- associated collection of feature variances (flux errors)
        X_lnorm -- ln(normalization) of multivariate Gaussian PDF
        wt_thresh -- threshold used to truncate the SOM PDF
        redchi2_thresh -- reduced-chi2 threshold to differentiate samples

        Outputs:
        som_map class with the following initialized parameters:
        cell_obj -- objects matched to each cell (after truncating PDF tails)
        cell_norm -- associated normalization (scalefactor) terms
        cell_shape -- associated chi2 quadratic (shapefactor) terms
        cell_chi2 -- associated chi2 values (OPTIMIZED over normalization)
        cell_chi2mod -- associated chi2 values (MARGINALIZED over normalization)
        cell_prob -- associated (normalized) SOM PDF
        cell_Nobj -- number of objects mapped to each cell
        cell_Nobj_clean -- as above, after applying obj_flag cuts
        cell_Nprob -- total likelihood contained within a given cell
        cell_Nprob_clean -- as above, after applying obj_flag cuts
        obj_bmu -- best-matching unit (cell)
        obj_bmu_dist -- distance from BMU
        obj_bestchi2 -- best-fit chi2 value
        obj_bestchi2mod -- best-fit chi2mod value
        obj_logZ -- ln(Bayesian evidence) per object over the SOM
        obj_flag -- flag based on reduced-chi2 threshold
        """

        Nobj=len(X)
        dof=self.Nfilt-1 # degrees of freedom

        # initialize cell lists
        cell_obj=[[] for i in xrange(self.Ncell)]
        cell_norm=[[] for i in xrange(self.Ncell)]
        cell_shape=[[] for i in xrange(self.Ncell)]
        cell_chi2=[[] for i in xrange(self.Ncell)]
        cell_chi2mod=[[] for i in xrange(self.Ncell)]
        cell_prob=[[] for i in xrange(self.Ncell)]
        
        # initialize object arrays
        obj_bmu=zeros(Nobj,dtype='int')
        obj_bmu_dist=zeros(Nobj,dtype='float32')
        obj_norm=zeros(Nobj,dtype='float32')
        obj_bestchi2=zeros(Nobj,dtype='float32')
        obj_bestchi2mod=zeros(Nobj,dtype='float32')
        obj_logZ=zeros(Nobj,dtype='float32')
        obj_flag=zeros(Nobj,dtype='bool')
        
        for i in xrange(Nobj): # for each object
            if i%10000==0: sys.stderr.write(str(i)+' ')

            # fit SOM cell models
            norm_vals,chi2_vals,shape_vals,chi2mod_vals=chi2(X[i],X_var[i],X_lnorm[i],self.cellmodel)
            midx=argmin(chi2_vals)
            obj_bmu[i]=midx # BMU
            obj_bmu_dist[i]=sum((norm_vals[midx]*self.cellmodel[midx]-X[i])**2) # squared distance from BMU
            obj_bestchi2[i]=chi2_vals[midx] # best-fit chi2
            obj_bestchi2mod[i]=min(chi2mod_vals) # best-fit modified chi2

            # convert from chi2 to PDF
            prob=exp(-0.5*(chi2mod_vals-obj_bestchi2mod[i])) # normalized probability
            prob_norm=sum(prob)
            obj_logZ[i]=log(prob_norm)-0.5*obj_bestchi2mod[i] # ln(Bayesian evidence)
            prob/=prob_norm # normalize

            # apply selection cut based on (1) reduced-chi2 flag and (2) distance flag
            if (obj_bestchi2mod[i]/dof<redchi2_thresh)|(obj_bmu_dist[i]<=self.cell_dNN[midx]):
                obj_flag[i]=True

            # sort objects
            sel_arr=arange(self.Ncell)[prob>wt_thresh]
            for j in sel_arr:
                cell_obj[j].append(i)
                cell_norm[j].append(norm_vals[j])
                cell_shape[j].append(shape_vals[j])
                cell_chi2[j].append(chi2_vals[j])
                cell_chi2mod[j].append(chi2mod_vals[j])
                cell_prob[j].append(prob[j])

        # re-format list entries
        for i in xrange(self.Ncell):
            cell_obj[i]=array(cell_obj[i],dtype='int')
            cell_norm[i]=array(cell_norm[i],dtype='float32')
            cell_shape[i]=array(cell_shape[i],dtype='float32')
            cell_chi2[i]=array(cell_chi2[i],dtype='float32')
            cell_chi2mod[i]=array(cell_chi2mod[i],dtype='float32')
            cell_prob[i]=array(cell_prob[i],dtype='float32')
            

        # initialize som_map object
        smap=som_map()

        smap.cell_obj=cell_obj
        smap.cell_norm=cell_norm
        smap.cell_shape=cell_shape
        smap.cell_chi2=cell_chi2
        smap.cell_chi2mod=cell_chi2mod
        smap.cell_prob=cell_prob
        smap.obj_bmu=obj_bmu
        smap.obj_bmu_dist=obj_bmu_dist
        smap.obj_bestchi2=obj_bestchi2
        smap.obj_bestchi2mod=obj_bestchi2mod
        smap.obj_logZ=obj_logZ
        smap.obj_flag=obj_flag

        smap.cell_Nobj=array([len(cell_obj[i]) for i in xrange(self.Ncell)],dtype='int')
        smap.cell_Nobj_clean=array([sum(obj_flag[cell_obj[i]]) for i in xrange(self.Ncell)],dtype='int')
        smap.cell_Nprob=array([sum(cell_prob[i]) for i in xrange(self.Ncell)],dtype='float32')
        smap.cell_Nprob_clean=array([sum(cell_prob[i][obj_flag[cell_obj[i]]]) for i in xrange(self.Ncell)],dtype='float32')

        return smap


    def map_chi2_subset(self, smap, sel):
        """
        Re-compute number and likelihood densities given a new selection array.

        Keyword arguments:
        smap -- som_map instance (see func::SOM.map_bmu)
        sel -- boolean selection array
        """

        cell_Nobj_new=array([sum(sel[smap.cell_obj[i]]) for i in xrange(self.Ncell)],dtype='int')
        cell_Nprob_new=array([sum(smap.cell_prob[i][sel[smap.cell_obj[i]]]) for i in xrange(self.Ncell)],dtype='float32')
        
        return cell_Nobj_new,cell_Nprob_new


    def pdf_chi2(self, cell_obj, cell_prob, y_pos, y_idx, ydict, ywidth, pdf_grid, delta_grid, Ngrid, sel=None):
        """
        Compute PDFs for objects given their corresponding SOM chi2 mappings.

        Keyword arguments:
        cell_obj -- object indices in each cell
        cell_prob -- associated likelihoods
        y_pos -- discretized index of target
        y_idx -- matching kernel (width) index
        ydict -- dictionary of Gaussian kernels
        ywidth -- width of kernels
        pdf_grid -- underlying grid used for fitting
        delta_grid -- spacing of grid
        Ngrid -- number of elements in grid
        sel -- boolean selection array of input objects

        Outputs:
        som_pdf -- stacked PDF computed within each som cell
        """

        # initialize variables
        som_pdf=zeros((self.Ncell,Ngrid),dtype='float32')

        # compute PDF(cell)
        for i in xrange(self.Ncell):
            if i%100==0: sys.stderr.write(str(i)+' ')
            cell_sel=cell_obj[i] # select objects matched to cell

            if len(cell_sel)>0: # if cell contains objects
                if sel is not None: # if boolean selection array given
                    sel2=sel[cell_sel]
                    if sum(sel2)>0: # if any objects remain after cuts
                        wt=cell_prob[i][sel2]
                        som_pdf[i]=pdf_kde_wt_dict(ydict,ywidth,y_pos[cell_sel][sel2],y_idx[cell_sel][sel2],wt,pdf_grid,delta_grid,Ngrid)/sum(wt)
                else: # if no selection array given
                    wt=cell_prob[i]
                    som_pdf[i]=pdf_kde_wt_dict(ydict,ywidth,y_pos[cell_sel],y_idx[cell_sel],wt,pdf_grid,delta_grid,Ngrid)/sum(wt)

        return som_pdf



    
















############ RE-WEIGHTING AND RE-SAMPLING THE SOM #############

class AdaptiveWeight():
    """
    Uses training/testing projections over the SOM (i.e. color space) to compute adaptive ensemble weights and related quantities.

    Functions:
    compute_weight -- compute relative weight
    compute_samples -- compute number of samples to be drawn from each SOM cell
    resample -- resample SOM cells
    """

    def __init__(self, cparams):
        """
        Initialize instance.

        Keyword arguments:
        cparams -- master configuration parameters
        """

        self.fmin=cparams['FMIN_RESAMPLE']
        self.fresamp=cparams['F_BOOTSTRAP']

        

    def compute_weight(self, som_pdf_train, som_pdf_test):
        """
        Compare the associated mappings between the training and test sets over the SOM to derive a series of adaptive weights to ameliorate train/test mismatches.

        Keyword arguments:
        som_pdf_train -- SOM PDF computed from the training sample
        som_pdf_test -- SOM PDF computed from the testing sample
        """

        # select all overlapping non-empty cells
        sel_arr=(som_pdf_train>0)&(som_pdf_test>0) 
        train_temp,test_temp=som_pdf_train[sel_arr],som_pdf_test[sel_arr]

        # normalize distributions
        train_norm,test_norm=sum(train_temp)*1.0,sum(test_temp)*1.0 
        train_pdf,test_pdf=train_temp/train_norm,test_temp/test_norm 

        # compute weights
        som_weights=ones(len(som_pdf_train)) # assuming no reweighting if no objects in cell
        som_weights[sel_arr]=test_pdf/train_pdf 
    
        self.weights=som_weights
        self.Nweights=len(som_weights)


    def compute_samples(self, som_pdf):
        """
        Return the number of objects that should be pulled from a given cell given SOM quantities, the minimum resampling threshold, and the resampling fraction.

        Keyword arguments: 
        som_pdf -- SOM PDF of the sample we are resampling from
        """

        Ncluster_resample=array(np.round(som_pdf*self.weights*self.fresamp),dtype='int') # number of objects to resample from each re-weighted cell
        Nmin=sort(Ncluster_resample)[int(round(self.fmin*len(som_pdf)))] # minimum number of objects included in a given cell
        Ncluster_resample[Ncluster_resample<Nmin]=Nmin # set minimum resampling floor
        Ncluster_resample[som_pdf==0]=0 # do not attempt to sample completely empty cells

        self.samples=Ncluster_resample
        self.Nsamples=sum(Ncluster_resample)
    
    

    def resample(self, cell_obj, cell_prob, frrf, sel=None, weights=None):
        """
        Resample objects from the SOM based on the adaptive weights.

        Keyword arguments:
        cell_obj -- object indices mathced to each cell
        cell_prob -- associated likelihoods
        frrf -- FRRF instance (see class::FRRF)
        sel -- object boolean selection array
        weights -- additional object weights (if other prior(s) desired)
        """

        indices=zeros((frrf.NTREES,self.Nsamples),dtype='int')

        for count in xrange(frrf.NTREES):
            sys.stderr.write(str(count)+' ')
      
            for i in arange(self.Nweights):

                cell_sel=cell_obj[i] # objects matched to cell
                wt_sel=cell_prob[i].astype('double') # associated weights

                low=int(sum(self.samples[:i])) # lower bound of indices slice
                high=int(low+self.samples[i]) # upper bound of indices slice

                if len(cell_sel)>0: # if cell contains objects
                    if sel is not None: # if boolean selection array given
                        sel2=sel[cell_sel]
                        if sum(sel2)>0: # if any objects remain after cuts
                            if weights is None: # if additional object weights passed
                                prob=wt_sel[sel2]/sum(wt_sel[sel2]) # normalize likelihood
                            else:
                                wt=weights[cell_sel]
                                prob=wt_sel[sel2]*wt[sel2]/sum(wt_sel[sel2]*wt[sel2])
                            indices[count][low:high]=random.choice(cell_sel[sel2],self.samples[i],replace=True,p=prob)
                        
                    else: # if no selection array given
                        if weights is None: # if no additional object weights passed
                            prob=wt_sel/sum(wt_sel) # normalize likelihood
                        else:
                            wt=weights[cell_sel]
                            prob=wt_sel*wt/sum(wt_sel*wt)
                        indices[count][low:high]=random.choice(cell_sel,self.samples[i],replace=True,p=prob)

        self.indices=indices.astype(int)








    






    


########### FULL REGRESSION RANDOM FOREST ###############


class FRRF():
    """
    The Full Regression Random Forest (FRRF) class. 

    Functions: 
    train -- trains the FRRF
    predict -- generates predictions
    """
    
    def __init__(self, fparams=None, N_members=100, criterion='mse', splitter='best', max_depth=None, min_samples_split=20, min_samples_leaf=10, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, presort=False):
        """
        Initializes the instance. 

        Keyword arguments:
        fparams -- FRRF parameters (from class::ReadParams)
        Additional arguments serve as inputs to the sklearn.tree.DecisionTreeRegressor class
        """

        # initialize parameters
        if fparams is not None:
            N_members=int(fparams['N_MODELS'])
            criterion=fparams['CRITERION']
            splitter=fparams['SPLITTER']
            max_depth=fparams['MAX_DEPTH']
            min_samples_split=int(fparams['MIN_SAMPLES_SPLIT'])
            min_samples_leaf=int(fparams['MIN_SAMPLES_LEAF'])
            min_weight_fraction_leaf=fparams['MIN_WEIGHT_FRACTION_LEAF']
            max_features=fparams['MAX_FEATURES']
            random_state=fparams['RANDOM_STATE']
            max_leaf_nodes=fparams['MAX_LEAF_NODES']
            presort=(fparams['PRESORT'] in ['True','TRUE'])

        # establish baseline model
        self.tree=tree.DecisionTreeRegressor(criterion=criterion,splitter=splitter,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf,max_features=max_features,random_state=random_state,max_leaf_nodes=max_leaf_nodes,presort=presort)

        # create forest
        self.forest=[base.clone(self.tree) for i in xrange(N_members)] # collection of tree deep copies
        self.NTREES=N_members # size

        # create tree-structured index
        self.forest_idx=None # indices
        self.forest_Nmax_pred=self.NTREES*(2*self.tree.min_samples_leaf-1)


    def train(self, X, Xe, y, ye, clean_idxs, outlier_idx):
        """
        Train the Full Regression Random Forest (FRRF) on the input data. 
        Errors are incorporated using Monte Carlo methods.

        Keyword arguments:
        X -- collection of features
        Xe -- associated collection of feature errors
        y -- matching collection of targets
        ye -- associated collection of target errors
        clean_idxs -- 'Nmember' sets of (resampled) indices of observations used in training EACH member
        outlier_idx -- set of outlier indices of observations used in training EVERY member
        """

        # initialize lists
        idxs=[]

        # train FRRF
        for i in xrange(self.NTREES):
            sys.stderr.write(str(i)+' ')

            # create Monte Carlo realization of data
            idx_t=append(clean_idxs[i],outlier_idx).astype('int') # indices (clean[i]+outlier)
            X_t=array(normal(X[idx_t],Xe[idx_t]),dtype='float32') # perturb features (mags)
            X_t=append(X_t,X_t[:,:-1]-X_t[:,1:],axis=1) # add linear combinations (colors)
            y_t=array(normal(y[idx_t],ye[idx_t]),dtype='float32') # perturbed features and targets

            # fit tree
            (self.forest)[i].fit(X_t,y_t) # copy and fit base model
            model_t_idx=(self.forest)[i].tree_.apply(X_t) # select corresponding nodes
            model_t_size=(self.forest)[i].tree_.node_count # total number of nodes

            # create a tree-structured list of training indices at each node
            model_t_idxs=[[] for counter in xrange(model_t_size)] # initialize data structure
            for counter in xrange(len(idx_t)):
                temp=model_t_idx[counter]
                model_t_idxs[temp].append(idx_t[counter]) # append object index to corresponding node list

            # add results to full list
            idxs.append(model_t_idxs) # indices

        self.forest_idx=idxs


    def predict(self, X_test, Xe_test, phot, err, phot_test, err_test, lnorm_test):
        """
        Generate predicted PDFs from the trained FRRF.
        Objects in the training set to be fit (i.e. fully regressed over using chi2) are selected based on a set of transformed FEATURES, while chi2 values are computed directly from the corresponding PHOTOMETRY. 
        Errors on the TRAINING AND TESTING data are incorporated using Monte Carlo methods.

        Keyword arguments:
        X_test -- collection of features in test set
        Xe_test -- associated errors of 'X_test'
        phot -- photometry of (full) training set
        err -- photometric errors of (full) training set
        phot_test -- photometry of testing set
        err_test -- photometric errors of testing set
        lnorm_test -- ln(normalization) of multivariate Gaussian PDF

        Outputs:
        model_objects,model_chi2mod_vals
        model_objects -- matched object indices
        model_chi2mod_vals -- associated chi2 values (MARGINALIZED over normalization)
        """

        # initialize stuff
        Ntrain,Ntest=len(phot),len(phot_test) # size of training and testing sets
    
        model_objects=zeros((Ntest,self.forest_Nmax_pred),dtype='int')-99 # collection of training object indices selected for each test object
        #model_norm_vals=zeros((Ntest,self.forest_Nmax_pred),dtype='float32') # maximum-likelihood normalizations (see func::chi2)
        #model_steep_vals=zeros((Ntest,self.forest_Nmax_pred),dtype='float32') # 'shapefactor' of chi2 scalefactor dependence (see func::chi2)
        #model_chi2_vals=zeros((Ntest,self.forest_Nmax_pred),dtype='float32') # chi2 (see func::chi2)
        model_chi2mod_vals=zeros((Ntest,self.forest_Nmax_pred),dtype='float32') # modified chi2 (see func::chi2)
    
        # locate corresponding nodes on trees for each input object
        sys.stderr.write('Finding best-matching nodes... ')
        objids=zeros((self.NTREES,Ntest),dtype='int') # initialize object index array
        for i in xrange(self.NTREES):
            sys.stderr.write(str(i)+' ') # counter
            X_test_t=array(normal(X_test,Xe_test),dtype='float32') # perturb features (mags)
            X_test_t=append(X_test_t,X_test_t[:,:-1]-X_test_t[:,1:],axis=1) # add linear combinations (colors)
            objids[i]=(self.forest)[i].tree_.apply(X_test_t) # nodes for each tree for all objects        
        objids=swapaxes(objids,0,1) # swap axes to get nodes for each object across all trees

    
        # chi2 fitting
        sys.stderr.write('Running full regression... ')
    
        for i in xrange(Ntest):
            modelobjs=[]

            # grab associated models (located at nodes of each tree)
            for j in xrange(self.NTREES):
                modelobjs+=(self.forest_idx)[j][objids[i][j]]
            Nmodel=len(modelobjs)

            # compute likelihoods
            modelphot_mc=array(normal(phot[modelobjs],err[modelobjs]),dtype='float32') # MC realization of features/targets
            ptest,etest,ltest=phot_test[i],err_test[i],lnorm_test[i] # test photometry
            norm_vals,chi2_vals,shape_vals,chi2_vals_mod=chi2(ptest,etest*etest,ltest,modelphot_mc) # chi2 fitting (see func::chi2)

            # add results to overall collection
            model_objects[i][:Nmodel]=copy(modelobjs).astype('int') # fitted object indices
            #model_norm_vals[i][:Nmodel]=copy(norm_vals) # maximum-likelihood normalizations (scalefactors)
            #model_shape_vals[i][:Nmodel]=copy(shape_vals) # chi2 quadratic coefficients (shapefactors)
            #model_chi2_vals[i][:Nmodel]=copy(chi2_vals).astype('float32') # chi2 OPTIMIZING over the scalefactor
            model_chi2mod_vals[i][:Nmodel]=copy(chi2_vals_mod).astype('float32') # chi2 MARGINALIZING over the scalefactor
                
            if i%5000==0: 
                sys.stderr.write(str(i)+' ') # counter
                gc.collect() # garbage collect
    
        sys.stderr.write('done!\n')

        return model_objects,model_chi2mod_vals





    



        
        


########### INPUT/OUTPUT OPERATIONS ###############


class ReadParams():
    """
    Read in configuration files and initialize parameters. [Code based on Gabriel Brammer's threedhst.eazyPy module.]
    """

    def __init__(self, config_file, config_dir='config/'):

        self.filename = config_file # filename

        # read in file
        f=open(config_file,'r')
        self.lines=f.readlines()
        f.close()

        # process file
        self._process_params()

        # process additional configuration files
        for param in self.param_names:
            if 'CONFIG_' in param:
                fname=self.params[param] # grab filename
                exec("self."+param+"=ReadParams(self.params['HOME']+config_dir+fname)") # assign ReadParams output to associated variable name


    def _process_params(self):
        """
        Process input parameters and add them to the class dictionary.
        """

        params={} # parameters
        formats={} # format of parameters
        self.param_names=[] # parameter names

        # extract parameters
        for line in self.lines:
            if (line.startswith('#') | line.startswith(' ')) is False:

                # split line
                lsplit=line.split()

                # assign name and parameter
                if len(lsplit)>=2:
                    lsplit[0]=lsplit[0][:-1]
                    params[lsplit[0]]=lsplit[1]
                    self.param_names.append(lsplit[0])

                    # (re)assign formats
                    try:
                        flt=float(lsplit[1])
                        formats[lsplit[0]]='f'
                        params[lsplit[0]]=flt
                    except:
                        formats[lsplit[0]]='s'

                    if params[lsplit[0]] == 'None':
                        params[lsplit[0]] = None
                        formats[lsplit[0]] = 'n'

        self.params=params
        self.formats=formats



class ReadFilters():
    """
    Read in filter files.
    """

    def __init__(self, filter_list, filter_dir='filters/', path='', Npoints=5e4):
        """
        Keyword arguments:
        filter_dir -- directory where filter files are stored
        path -- home path
        Npoints -- number of points used to interpolate filter transmission curves
        """

        c=299792458.0 # speed of light in m/s
        
        f=open(path+filter_dir+filter_list)
        self.filters=[]
        self.filenames=[]
        for line in f:
            lsplit = line.split()
            self.filters.append(lsplit[0])
            self.filenames.append(lsplit[1])
        f.close()

        self.NFILTER=len(self.filters)
        
        self.fw=[0.]*self.NFILTER
        self.ft=[0.]*self.NFILTER

        for i in xrange(self.NFILTER):
            self.fw[i],self.ft[i]=swapaxes(loadtxt(path+filter_dir+self.filenames[i]),0,1)

        self.lambda_eff=zeros(self.NFILTER)

        for i in xrange(self.NFILTER):
            nuMax=0.999*c/(min(self.fw[i])*1e-10) # max frequency
            nuMin=1.001*c/(max(self.fw[i])*1e-10) # min frequency
            nuInc=(nuMax-nuMin)/Npoints # increment (linear)
            nu=arange(nuMin,nuMax+nuInc,nuInc) # frequency array
            lnu=log(nu)
            wave=c/nu # convert to lambda
            lwave=log(wave)

            func=interpolate.interp1d(self.fw[i],self.ft[i],kind='linear') # spline filter
            temp=func(1e10*wave) # transmission (in Angstroms)

            top=trapz(temp*lwave,lnu)
            bottom=trapz(temp,lnu)
            self.lambda_eff[i]=exp(top/bottom)*1e10 # effective wavelength of filter



class TrainCatalog():
    """
    Read input TRAINING catalog.
    """

    def __init__(self, cparams, rdict, mdict, cdict):
        """
        Keyword arguments:
        cparams -- master configuration parameters
        rdict -- redshift dictionary (see class::RedshiftDict)
        mdict -- magnitude dictionary (see class::PDFDict)
        cdict -- color dictionary (see class::PDFDict)
        """

        # header:
        # ra2000 decl2000 gflux_cmodel rflux_cmodel iflux_cmodel zflux_cmodel yflux_cmodel gflux_cmodel_err rflux_cmodel_err iflux_cmodel_err zflux_cmodel_err yflux_cmodel_err zbest_source zbest zl68 zh68 survey

        # training catalog
        catalog=loadtxt(cparams['DATA_TRAIN'])
        catsize=len(catalog)

        # metadata
        self.objid=arange(catsize) # object ID
        self.ra=catalog[:,0] # RA
        self.dec=catalog[:,1] # DEC

        # photometry
        self.phot=catalog[:,[2,3,4,5,6]]*(1e23*1e6) # in cgs (de-reddened)
        err0=catalog[:,[7,8,9,10,11]]*(1e23*1e6) # in cgs (de-reddened)
        self.err=sqrt(err0**2+(cparams['SYS_ERR_TRAIN']*self.phot)**2) # add in systematic error
        self.lnorm=log(sum(self.err**2,axis=1))+len(self.phot[0])*l2pi # ln(normalization)

        # skynoise
        if os.path.isfile(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TRAIN']):
            b=loadtxt(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TRAIN']) # load skynoise file
        else:
            b=median(err0,axis=0) # compute skynoise from data
            savetxt(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TRAIN'],b)
        self.skynoise=b

        # asinh mags and colors
        self.mag_asinh=-2.5/log(10)*(arcsinh(self.phot/(2*b))+log(self.skynoise/cparams['ZEROPOINT_TRAIN']))
        self.mag_asinh_err=sqrt((2.5*log10(e)*self.err)**2/(4*self.skynoise**2+self.phot**2))
        self.color_asinh=self.mag_asinh[:,:-1]-self.mag_asinh[:,1:]
        self.color_asinh_err=sqrt(self.mag_asinh_err[:,:-1]**2+self.mag_asinh_err[:,1:]**2)

        # evaluate on magnitude and redshift grids
        self.mag_asinh_idx=np.round((self.mag_asinh-mdict.grid[0])/mdict.delta).astype('int')
        self.mag_asinh_err_idx=np.round((self.mag_asinh_err-mdict.sig_grid[0])/mdict.dsig).astype('int')
        self.color_asinh_idx=np.round((self.color_asinh-cdict.grid[0])/cdict.delta).astype('int')
        self.color_asinh_err_idx=np.round((self.color_asinh_err-cdict.sig_grid[0])/cdict.dsig).astype('int')

        self.mag_asinh_err_idx[self.mag_asinh_err_idx>=len(mdict.sig_grid)]=len(mdict.sig_grid)-1 # error bounds
        self.color_asinh_err_idx[self.color_asinh_err_idx>=len(cdict.sig_grid)]=len(cdict.sig_grid)-1 # error bound
        
        # redshift info
        self.redshift=catalog[:,-4]
        self.redshift_err=np.max(c_[self.redshift-catalog[:,-3],catalog[:,-2]-self.redshift],axis=1)
        self.redshift_err[(catalog[:,-3]<=0)|(catalog[:,-2]<=0)]=1e-3

        # log(1+z) info
        self.lz=log(1+self.redshift)
        lz_err0=self.redshift_err/(1+self.redshift)
        self.lz_err=sqrt(lz_err0**2+rdict.dlz**2) # add grid resolution in quadrature

        # evaluate on redshift dictionary
        self.lz_idx=np.round((self.lz-rdict.lzgrid_highres[0])/rdict.dlz_highres).astype('int')
        self.lz_err_idx=np.round((self.lz_err-rdict.lze_grid[0])/rdict.dlze).astype('int')

        # flags
        self.flag_class=catalog[:,-5] # 1=specz, 2=grismz, 3=photoz
        self.flag_survey=catalog[:,-1] # see header for details

        # selection criteria (bitwise flag)
        clean_sel=array((sum(self.err<=0,axis=1)==0)==False,dtype='int8')*1 # measured in all 5 bands
        iband_sel=array((self.mag_asinh[:,2]<25.2)==False,dtype='int8')*2 # i<25.2 cut
        color_sel=array(
            (((self.mag_asinh[:,0]-self.mag_asinh[:,1])>-0.4) &
            ((self.mag_asinh[:,0]-self.mag_asinh[:,1])<2.7) &
            ((self.mag_asinh[:,1]-self.mag_asinh[:,2])>-0.2) &
            ((self.mag_asinh[:,1]-self.mag_asinh[:,2])<1.7) &
            ((self.mag_asinh[:,2]-self.mag_asinh[:,3])>-0.5) &
            ((self.mag_asinh[:,2]-self.mag_asinh[:,3])<1.2) &
            ((self.mag_asinh[:,3]-self.mag_asinh[:,4])>-1) &
            ((self.mag_asinh[:,3]-self.mag_asinh[:,4])<1.1)) == False
            ,dtype='int8')*4 # ad hoc color cuts
        measurement_sel=array(
            ((sum(isnan(self.phot),axis=1)==0) & (sum(isnan(self.err),axis=1)==0) &
            (sum(isinf(self.phot),axis=1)==0) & (sum(isinf(self.err),axis=1)==0) &
            (sum(self.phot==0,axis=1)==0) & (sum(self.err<=0,axis=1)==0) &
            (sum(isnan(self.mag_asinh),axis=1)==0) & (sum(isnan(self.mag_asinh_err),axis=1)==0) &
            (sum(isinf(self.mag_asinh),axis=1)==0) & (sum(isinf(self.mag_asinh_err),axis=1)==0) &
            (sum(self.mag_asinh==0,axis=1)==0) & (sum(self.mag_asinh_err<=0,axis=1)==0)) == False
            ,dtype='int8')*8 # removing bad photometry
        redshift_sel=array((self.lz_err<cparams['ZDISP_SEL'])==False,dtype='int8')*16 # removing insecure redshifts

        # training set selection
        self.flag_quality=clean_sel+iband_sel+color_sel+measurement_sel+redshift_sel
        self.flag_train=(self.flag_quality==0)

        # number of objects in catalog
        self.NCAT=catsize
        self.NTRAIN=sum(self.flag_train)




class TestCatalog():
    """
    Read input TESTING catalog.
    """

    def __init__(self, cparams, mdict, cdict):
        """
        Keyword arguments:
        cparams -- master configuration file parameters
        """

        # loading testing data
        f=fits.open(cparams['DATA_TEST'])
        catalog=f[1].data
        catsize=len(catalog)
        f.close()
    
        # metadata
        self.objid=catalog['id']
        self.ra=catalog['ra2000']
        self.dec=catalog['decl2000']

        # photometry
        phot=c_[catalog['gflux_cmodel'],
                catalog['rflux_cmodel'],
                catalog['iflux_cmodel'],
                catalog['zflux_cmodel'],
                catalog['yflux_cmodel']]*(1e23*1e6) # in uJy
        err0=c_[catalog['gflux_cmodel_err'],
               catalog['rflux_cmodel_err'],
               catalog['iflux_cmodel_err'],
               catalog['zflux_cmodel_err'],
               catalog['yflux_cmodel_err']]*(1e23*1e6) # in uJy
        ebv=10**(-0.4*c_[catalog['a_g'],
                         catalog['a_r'],
                         catalog['a_i'],
                         catalog['a_z'],
                         catalog['a_y']]) # dust correction (multiplicative)

        # apply dust correction
        self.phot=phot*ebv
        err0=err0*ebv

        # systematic errors
        self.err=sqrt(err0**2+(cparams['SYS_ERR_TEST']*self.phot)**2) # systematic calibration uncertainty

        # multivariate gaussian normalization
        self.lnorm=log(sum(self.err**2,axis=1))+len(self.phot[0])*l2pi # ln(normalization)

        # skynoise
        if os.path.isfile(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TEST']):
            b=loadtxt(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TEST']) # load skynoise file
        else:
            b=median(err0,axis=0) # compute skynoise from data
            savetxt(cparams['HOME']+'skynoise/'+cparams['SKYNOISE_TEST'],b)
        self.skynoise=b

        # asinh mags    
        self.mag_asinh=-2.5/log(10)*(arcsinh(self.phot/(2*b))+log(self.skynoise/cparams['ZEROPOINT_TEST']))
        self.mag_asinh_err=sqrt((2.5*log10(e)*self.err)**2/(4*self.skynoise**2+self.phot**2))
        self.color_asinh=self.mag_asinh[:,:-1]-self.mag_asinh[:,1:]
        self.color_asinh_err=sqrt(self.mag_asinh_err[:,:-1]**2+self.mag_asinh_err[:,1:]**2)

        # evaluate on magnitude and redshift grids
        self.mag_asinh_idx=np.round((self.mag_asinh-mdict.grid[0])/mdict.delta).astype('int')
        self.mag_asinh_err_idx=np.round((self.mag_asinh_err-mdict.sig_grid[0])/mdict.dsig).astype('int')
        self.color_asinh_idx=np.round((self.color_asinh-cdict.grid[0])/cdict.delta).astype('int')
        self.color_asinh_err_idx=np.round((self.color_asinh_err-cdict.sig_grid[0])/cdict.dsig).astype('int')

        self.mag_asinh_err_idx[self.mag_asinh_err_idx>=len(mdict.sig_grid)]=len(mdict.sig_grid)-1 # error bounds
        self.color_asinh_err_idx[self.color_asinh_err_idx>=len(cdict.sig_grid)]=len(cdict.sig_grid)-1 # error bound

        # selection criteria (bitwise flag)
        clean_sel=array((sum(self.err<=0,axis=1)==0)==False,dtype='int8')*1 # measured in all 5 bands
        iband_sel=array((self.mag_asinh[:,2]<25.2)==False,dtype='int8')*2 # i<25.2 cut
        color_sel=array(
            (((self.mag_asinh[:,0]-self.mag_asinh[:,1])>-0.4) &
            ((self.mag_asinh[:,0]-self.mag_asinh[:,1])<2.7) &
            ((self.mag_asinh[:,1]-self.mag_asinh[:,2])>-0.2) &
            ((self.mag_asinh[:,1]-self.mag_asinh[:,2])<1.7) &
            ((self.mag_asinh[:,2]-self.mag_asinh[:,3])>-0.5) &
            ((self.mag_asinh[:,2]-self.mag_asinh[:,3])<1.2) &
            ((self.mag_asinh[:,3]-self.mag_asinh[:,4])>-1) &
            ((self.mag_asinh[:,3]-self.mag_asinh[:,4])<1.1)) == False
            ,dtype='int8')*4 # ad hoc color cuts
        measurement_sel=array(
            ((sum(isnan(self.phot),axis=1)==0) & (sum(isnan(self.err),axis=1)==0) &
            (sum(isinf(self.phot),axis=1)==0) & (sum(isinf(self.err),axis=1)==0) &
            (sum(self.phot==0,axis=1)==0) & (sum(self.err<=0,axis=1)==0) &
            (sum(isnan(self.mag_asinh),axis=1)==0) & (sum(isnan(self.mag_asinh_err),axis=1)==0) &
            (sum(isinf(self.mag_asinh),axis=1)==0) & (sum(isinf(self.mag_asinh_err),axis=1)==0) &
            (sum(self.mag_asinh==0,axis=1)==0) & (sum(self.mag_asinh_err<=0,axis=1)==0)) == False
            ,dtype='int8')*8 # removing bad photometry

        # training set selection
        self.flag_quality=clean_sel+iband_sel+color_sel+measurement_sel
        self.flag_test=(self.flag_quality==0)

        # number of objects in catalog
        self.NCAT=catsize
        self.NTEST=sum(self.flag_test)










        
    
        

########### DICTIONARIES ###############


class RedshiftDict():
    """
    Set up redshift grids and kernels used for computations.
    """

    
    def __init__(self, cparams, rparams, sigma_trunc=5.0):
        """
        Set up redshift grids/kernels.

        Keyword arguments:
        cparams -- master configuration parameters (see class:ReadParams)
        rparams -- redshift configuration parameters (see class::ReadParams)
        sigma_trunc -- +/-sigma_trunc standard deviations used to evaluate the kernels
        """

        # discrete kernel parameters
        self.lze_grid=linspace(rparams['DLZ'],cparams['ZDISP_SEL'],int(rparams['N_DICT'])) # Gaussian dictionary parameter grid
        self.dlze=self.lze_grid[1]-self.lze_grid[0] # kernel spacing

        # high-res log(1+z) grid
        self.res=rparams['RES']
        self.dlz_highres=rparams['DLZ']/rparams['RES'] # high-res spacing
        self.Npad=int(sigma_trunc*cparams['ZDISP_SEL']/self.dlz_highres) # padding on ends of grid (for sliding addition)
        self.lzgrid_highres=arange(log(1+rparams['ZMIN']),log(1+rparams['ZMAX'])+self.dlz_highres,self.dlz_highres) # high-res grid
        self.lzgrid_highres=append(arange(log(1+rparams['ZMIN'])-self.dlz_highres*self.Npad,log(1+rparams['ZMIN']),self.dlz_highres),self.lzgrid_highres) # left-pad
        self.lzgrid_highres=append(self.lzgrid_highres,arange(log(1+rparams['ZMAX'])+self.dlz_highres,log(1+rparams['ZMAX'])+self.dlz_highres*(self.Npad+1),self.dlz_highres)) # right-pad
        self.Nz_highres=len(self.lzgrid_highres) # size of grid

        # effective bounds of high-res grid
        self.zmin_idx_highres=argmin(abs(self.lzgrid_highres-log(1+rparams['ZMIN'])))
        self.zmax_idx_highres=argmin(abs(self.lzgrid_highres-log(1+rparams['ZMAX'])))
        self.zmax_idx_highres=self.zmin_idx_highres+int(ceil((self.zmax_idx_highres-self.zmin_idx_highres)/rparams['RES'])*rparams['RES'])

        # lower-res log(1+z) grid
        self.dlz=rparams['DLZ']
        self.lzgrid=arange(self.lzgrid_highres[self.zmin_idx_highres],self.lzgrid_highres[self.zmax_idx_highres],self.dlz)
        self.Nz=len(self.lzgrid)

        # corresponding redshift grids
        self.zgrid=exp(self.lzgrid)-1 # low-res
        self.znorm=(self.zgrid[1]-self.zgrid[0])*self.zgrid+(self.zgrid[1]-self.zgrid[0]) # normalizations (for conversions from log(1+z) to z)
        self.znorm/=self.znorm[0]

        # create dictionary
        self.lze_width=ceil(self.lze_grid*sigma_trunc/self.dlz_highres).astype('int') # width of kernel
        self.lze_dict=[gaussian(self.lzgrid_highres[self.Nz_highres/2],self.lze_grid[i]**2,self.lzgrid_highres[self.Nz_highres/2-self.lze_width[i]:self.Nz_highres/2+self.lze_width[i]+1]) for i in xrange(int(rparams['N_DICT']))]
        self.Ndict=len(self.lze_dict)
        
        # output redshift grid
        self.zgrid_out=arange(rparams['ZMIN_OUT'],rparams['ZMAX_OUT']+rparams['DZ_OUT'],rparams['DZ_OUT'])
        self.dz_out=rparams['DZ_OUT']
        self.dz_out_highres=rparams['DZ_OUT']/rparams['RES_OUT']
        self.Nz_out=len(self.zgrid_out)
        

class PDFDict():
    """
    Set up underlying grids for general parameters.
    """

    
    def __init__(self, pparams, sigma_trunc=5.0):
        """
        Keyword arguments:
        pparams -- configuration parameters for the PDF file (see class:ReadParams)
        """

        # initialize grid
        self.delta=pparams['DELTA']
        self.min=pparams['MIN']
        self.max=pparams['MAX']
        self.grid=arange(self.min,self.max+self.delta/2,self.delta)
        self.Ngrid=len(self.grid)
        
        # create dictionary
        self.sig_grid=linspace(pparams['SIG_MIN'],pparams['SIG_MAX'],int(pparams['N_DICT'])) # Gaussian dictionary parameter grid
        self.dsig=self.sig_grid[1]-self.sig_grid[0] # kernel spacing
        self.sig_width=ceil(self.sig_grid*sigma_trunc/self.delta).astype('int') # width of kernel
        self.sig_dict=[gaussian(self.grid[self.Ngrid/2],self.sig_grid[i]**2,self.grid[self.Ngrid/2-self.sig_width[i]:self.Ngrid/2+self.sig_width[i]+1]) for i in xrange(int(pparams['N_DICT']))]        


















################ PLOTTING ################


def plot_som(som,weight_var,varname='function',colorscheme='jet',clim=None):
    """
    Create a 2D histogram on the SOM of the chosen variable.

    Keyword arguments:
    som -- SOM instance from class::SOM
    weight_var -- weight variable for each cell
    varname -- name of weight variable
    colorscheme -- chosen colorscheme for plotting
    clim -- color limits (used in conjunction with colorscheme)

    Outputs: 
    H -- 2-D (weighted) histogram
    """
    
    H,xedges,yedges=histogram2d(som.position[:,0]+0.5,som.position[:,1]+0.5,bins=[arange(0,som.dimensions[0]+1),arange(0,som.dimensions[1]+1)],weights=weight_var) # 2-D histogram

    H=ma.masked_array(H,mask=(H==0)+isnan(H)) # mask array

    # establish colormap
    cmap=get_cmap(colorscheme)
    cmap.set_bad('gray')

    # plot SOM
    imshow(swapaxes(H,0,1), interpolation='nearest', origin='lower', extent=[xedges[0],xedges[-1],yedges[0],yedges[-1]],cmap=cmap)
    xlabel('X',color='k',labelpad=12)
    ylabel('Y',color='k',labelpad=12)
    cbar=colorbar()
    cbar.set_label(varname,rotation=270,labelpad=50,color='k')
    if clim is not None: cbar.set_clim(clim) # color limits

    return H


def plot_cell(pos, filt, som, som_map, cat_train, rdict, mdict, cdict, midx=2, cidx=0, sel_list=None, Nresample=10, color_cell='blue', pdfcolor='orange', rparams=[0,6,1.0,0], mparams=[18,26,2.0,0], cparams=[0,2,0.4,0]):
    """
    Plot SOM cell contents. Inclues photometric, redshift, magnitude, and color distributions.

    Keyword arguments:
    pos -- SOM cell position
    filt -- filter object
    som -- SOM object
    som_map -- som_map object
    cat_train -- training catalog object
    [x]dict -- dictionary object (r=redshift, m=magnitude, c=color)
    [x]idx -- multidimensional slice (m=magnitude, c=color)
    sel_list -- list of boolean selection arrays
    Nresample -- number of samples to draw from each object's PDF
    color_cell -- color of the cell model
    pdfcolor -- color(s) of violin plot PDF
    [x]params -- parameters for plotting [min, max, delta, smoothing kernel]
    """
    
    # breakdown of plots
    gs = gridspec.GridSpec(3,3)
    zmax,mmax,cmax=[],[],[] # maximum bound of plots

    # number of plotting instances
    Nplot=1
    if sel_list is not None:
        Nplot=len(sel_list)

    # 1-D index of position
    pos_idx=arange(som.Ncell)[product(som.position==pos,axis=1)==1].item()

    # selection of objects that pass initial cuts
    cell_sel=som_map.obj_flag[som_map.cell_obj[pos_idx]] # selection array
    cell_obj=som_map.cell_obj[pos_idx][cell_sel] # selected objects
    cell_prob=(som_map.cell_prob[pos_idx][cell_sel]).astype('double') # associated probability
    cell_norm=(som_map.cell_norm[pos_idx][cell_sel]).astype('double') # associated normalization

    # cell photometry
    cell_phot=cat_train.phot[cat_train.flag_train]
    cell_err=cat_train.err[cat_train.flag_train]

    # SOM cell model
    sphot=som.cellmodel[pos_idx]
    snorm=max(sphot)

    # dimensions
    xmin,xmax=min(filt.lambda_eff),max(filt.lambda_eff)
    ymin,ymax=-0.2,1.3
    
    for count in xrange(Nplot):

        if Nplot > 1:
            pcolor=pdfcolor[count]
        else:
            pcolor=pdfcolor

        # applying additional selection criteria
        if sel_list is not None:
            sel=(sel_list[count])[cell_obj]
            tobj=cell_obj[sel]
            tprob=cell_prob[sel]
            tnorm=cell_norm[sel]
        else:
            tobj=cell_obj
            tprob=cell_prob
            tnorm=cell_norm

        # training photometry in cell
        tphot=cell_phot[tobj]
        terr=cell_err[tobj]
        
        # resample photometry
        Nobj=len(tobj)
        idx_resample=random.choice(Nobj,Nobj*Nresample,p=tprob/sum(tprob))
        
        # perturb photometry
        vals=normal(tphot[idx_resample],terr[idx_resample])
        vals/=(snorm*tnorm[idx_resample][:,None]) # normalize
        
        # plot cell photometry
        subplot(gs[:2,:])
        yticks(arange(-10.0,10.0,0.1))
        plot(filt.lambda_eff,sphot/snorm,'s-',color=color_cell,markersize=15)
        xlabel('Wavelength [A]',labelpad=15)
        ylabel('Normalized Flux',labelpad=10)
        xlim([xmin-0.025*xmax,1.025*xmax])
        ylim([ymin,ymax])
        
        # plot photometric distribution
        for i in xrange(filt.NFILTER):
            vt=vals[:,i] # per filter
            vt_m,vt_s=mean(vt),std(vt) # mean,std
            violin_parts=violinplot(vt[(vt>=(vt_m-2*vt_s))&(vt<=(vt_m+2*vt_s))],[filt.lambda_eff[i]],widths=600,showmeans=False,showmedians=False,showextrema=False) # violin plot
            for pc in violin_parts['bodies']: # change color
                pc.set_facecolor(pcolor)
                pc.set_edgecolor('black')

        tight_layout()

        # redshift pdf
        lz_pdf=pdf_kde_wt_dict(rdict.lze_dict,rdict.lze_width,cat_train.lz_idx[cat_train.flag_train][tobj],cat_train.lz_err_idx[cat_train.flag_train][tobj],tprob,rdict.lzgrid_highres,rdict.dlz_highres,rdict.Nz_highres)
        z_pdf=lz_pdf[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]/rdict.znorm
        zlow,zhigh=argmin(abs(rdict.zgrid-rparams[0])),argmin(abs(rdict.zgrid-rparams[1]))
        zgrid_temp=rdict.zgrid[zlow:zhigh+1]
        z_pdf=z_pdf[zlow:zhigh+1]
        if rparams[-1]>0:
            z_pdf=sum([z_pdf[i]*gaussian(zgrid_temp[i],rparams[-1]**2,zgrid_temp) for i in xrange(len(zgrid_temp))],axis=0)
        z_pdf/=trapz(z_pdf,zgrid_temp)
        zmax.append(max(z_pdf))

        # plot redshift pdf
        subplot(gs[2,0])
        xticks(arange(rparams[0],rparams[1]+rparams[2]/2.0,rparams[2]),fontsize=24)
        yticks([])
        xlim([rparams[0],rparams[1]])
        ylim([0,max(zmax)*1.05])
        plot(zgrid_temp,z_pdf,color='black',lw=0.25)
        fill_between(zgrid_temp,z_pdf,color=pcolor,alpha=0.7)
        xlabel('Redshift')
        tight_layout()

        # mag pdf
        mag_pdf=pdf_kde_wt_dict(mdict.sig_dict,mdict.sig_width,cat_train.mag_asinh_idx[:,midx][cat_train.flag_train][tobj],cat_train.mag_asinh_err_idx[:,midx][cat_train.flag_train][tobj],tprob,mdict.grid,mdict.delta,mdict.Ngrid)
        mlow,mhigh=argmin(abs(mdict.grid-mparams[0])),argmin(abs(mdict.grid-mparams[1]))
        mgrid_temp=mdict.grid[mlow:mhigh+1]
        mag_pdf=mag_pdf[mlow:mhigh+1]
        if mparams[-1]>0:
            mag_pdf=sum([mag_pdf[i]*gaussian(mgrid_temp[i],mparams[-1]**2,mgrid_temp) for i in xrange(len(mgrid_temp))],axis=0)
        mag_pdf/=trapz(mag_pdf,mgrid_temp)
        mmax.append(max(mag_pdf))

        # plot mag pdf
        subplot(gs[2,1])
        xticks(arange(mparams[0],mparams[1]+mparams[2]/2.0,mparams[2]),fontsize=24)
        yticks([])
        xlim([mparams[0],mparams[1]])
        ylim([0,max(mmax)*1.05])
        plot(mgrid_temp,mag_pdf,color='black',lw=0.25)
        fill_between(mgrid_temp,mag_pdf,color=pcolor,alpha=0.7)
        xlabel('Magnitude ('+filt.filters[midx]+')')
        tight_layout()
        
        # color pdf
        color_pdf=pdf_kde_wt_dict(cdict.sig_dict,cdict.sig_width,cat_train.color_asinh_idx[:,cidx][cat_train.flag_train][tobj],cat_train.color_asinh_err_idx[:,cidx][cat_train.flag_train][tobj],tprob,cdict.grid,cdict.delta,cdict.Ngrid)
        clow,chigh=argmin(abs(cdict.grid-cparams[0])),argmin(abs(cdict.grid-cparams[1]))
        cgrid_temp=cdict.grid[clow:chigh+1]
        color_pdf=color_pdf[clow:chigh+1]
        if cparams[-1]>0:
            color_pdf=sum([color_pdf[i]*gaussian(cgrid_temp[i],cparams[-1]**2,cgrid_temp) for i in xrange(len(cgrid_temp))],axis=0)
        color_pdf/=trapz(color_pdf,cgrid_temp)
        cmax.append(max(color_pdf))

        # plot color pdf
        subplot(gs[2,2])
        xticks(arange(cparams[0],cparams[1]+cparams[2]/2.0,cparams[2]),fontsize=24)
        yticks([])
        xlim([cparams[0],cparams[1]])
        ylim([0,max(cmax)*1.05])
        plot(cgrid_temp,color_pdf,color='black',lw=0.25)
        fill_between(cgrid_temp,color_pdf,color=pcolor,alpha=0.7)
        xlabel('Color ('+filt.filters[cidx]+'-'+filt.filters[cidx+1]+')')
        tight_layout()

    # detail cell quantities
    subplot(gs[:2,:])
    text(xmin-0.025*xmax+(xmax-xmin)*0.025,ymax-(ymax-ymin)*0.07,str(pos),weight='bold',color=color_cell,fontsize=30)
    text(xmin-0.025*xmax+(xmax-xmin)*0.025,ymax-(ymax-ymin)*0.15,'P(cell)='+str(round(sum(cell_prob),2)),weight='bold',color='red',fontsize=30)
    text(xmin-0.025*xmax+(xmax-xmin)*0.025,ymax-(ymax-ymin)*0.23,str(len(cell_prob))+' objects',weight='bold',color='black',fontsize=30)
    tight_layout()


def plot_cell_2dpz(pos, filt, som, som_map, cell_lz_idx, cell_lze_idx, rdict, cell_p_idx, cell_pe_idx, pdict, pname, sel_list=None, rparams=[0,6,1.0], pparams=[18,26,2.0], rnames=None, pdf_thresh=1e-3, boxcar=10):
    """
    Plot 2-D SOM cell P(z) vs input parameter dictionary.

    Keyword arguments:
    pos -- SOM position
    filt -- filter object
    som -- SOM object
    som_map -- som_map object
    cell_lz_idx -- log(1+z) discretized indices
    cell_lze_idx -- log(1+z) dictionary indoces
    rdict -- redshift dictionary
    cell_p_idx -- parameter discretized indices
    cell_pe_idx -- parameter dictionary indices
    pdict -- parameter dictionary
    pname -- parameter name
    boxcar -- width (in grid units) of smoothing boxcar
    See func::plot_cell for additional arguments.

    Ouptuts:
    2d_stack -- 2-D stacked PDF
    """

    # define number of plotting instances
    Nplot=1
    if sel_list is not None:
        Nplot=len(sel_list)

    # define breakdown of plots
    gs = gridspec.GridSpec(Nplot,Nplot)

    # 1-D index of position
    pos_idx=arange(som.Ncell)[product(som.position==pos,axis=1)==1].item()

    # selection of objects that pass initial cuts
    cell_sel=som_map.obj_flag[som_map.cell_obj[pos_idx]] # selection array
    cell_obj=som_map.cell_obj[pos_idx][cell_sel] # selected objects
    cell_prob=(som_map.cell_prob[pos_idx][cell_sel]).astype('double') # associated probability

    rmean_arr,weight_arr=[],[]
    
    for count in xrange(Nplot):

        temp_stack=zeros((pdict.Ngrid,rdict.Nz_highres)) # 2-D P(z) grid

        # applying additional selection criteria
        if sel_list is not None:
            sel=(sel_list[count])[cell_obj]
            tobj=cell_obj[sel]
            tprob=cell_prob[sel]
        else:
            tobj=cell_obj
            tprob=cell_prob

        # photometry in cell
        tphot=cell_p_idx[tobj]
        terr=cell_pe_idx[tobj]

        # redshift in cell
        tlz=cell_lz_idx[tobj]
        tlze=cell_lze_idx[tobj]

        # stack multivariate Gaussian
        for i in xrange(len(tobj)):
            x_cent,y_cent=tphot[i],tlz[i]
            x_bound,y_bound=pdict.sig_width[terr[i]],rdict.lze_width[tlze[i]]
            mvg=outer(pdict.sig_dict[terr[i]],rdict.lze_dict[tlze[i]])*tprob[i]
            temp_stack[x_cent-x_bound:x_cent+x_bound+1,y_cent-y_bound:y_cent+y_bound+1]+=mvg
        temp_stack=temp_stack[:,rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]
        #temp_stack/=rdict.znorm

        rmean_arr.append(average(rdict.lzgrid,weights=sum(temp_stack,axis=0))) # mean redshift averaged across the whole sample
        weight_arr.append(sum(tprob))
        
        # running median
        rmed_sliding=zeros(pdict.Ngrid)
        for i in xrange(pdict.Ngrid):
            if sum(temp_stack[i,:])==0:
                rmed_sliding[i]=NaN
            else:
                temp=cumsum(temp_stack[i,:])
                temp/=temp[-1]
                rmed_sliding[i]=rdict.lzgrid[argmin(abs(temp-0.5))]
        
        temp_stack=ma.array(temp_stack,mask=temp_stack<pdf_thresh) # mask array

        # plot 2-D stack
        subplot(gs[:,count])
        imshow(temp_stack,origin='lower',extent=(rdict.lzgrid[0],rdict.lzgrid[-1],pdict.grid[0],pdict.grid[-1]),aspect='auto',norm=matplotlib.colors.LogNorm(vmin=None, vmax=None))
        plot(pdict.grid*0.0+rmean_arr[count],pdict.grid,'r-.',lw=2)
        plot(convolve(rmed_sliding[isnan(rmed_sliding)==False],ones(boxcar).astype('float')/boxcar,'valid'),convolve(pdict.grid[isnan(rmed_sliding)==False],ones(boxcar).astype('float')/boxcar,'valid'),'k-',lw=2,zorder=10)
        colorbar(label='PDF')
        zticks=arange(rparams[0],rparams[1]+rparams[2],rparams[2])
        lzticks=log(1+zticks)
        xticks(lzticks,zticks)
        xlim([lzticks[0],lzticks[-1]])
        yticks(arange(pparams[0],pparams[1]+pparams[2],pparams[2]))
        ylim([pparams[0],pparams[1]])
        if rnames is not None:
            xlabel(rnames[count])
        else:
            xlabel('Redshift')
        ylabel(pname)
        title('N_obj='+str(round(sum(tprob),2)),y=1.02)
        tight_layout()

    # plot mean average across all samples
    rmean=average(rmean_arr,weights=weight_arr)
    for count in xrange(Nplot):
        subplot(gs[:,count])
        plot(pdict.grid*0.0+rmean,pdict.grid,'r--',lw=3)

    return temp_stack


def plot_nz(sample_names,train_nz,out_nz,zgrid,deltaz,zrange=[0,6],out_nz_draws=None):
    """
    Plot comparison between two N(z) (or two general number density) distributions.

    Keyword arguments:
    sample_names -- names for each sample
    train_nz -- original N(z)
    out_nz -- comparison N(z)
    zgrid -- grid N(z) is evaluated on
    deltaz -- dz spacing
    zrange -- plotting range

    Outputs:
    Identical to func::compute_density_score.
    """

    # compute density scores
    [N_p,arr_p,arr_prob_p,z_sel],N_ks=compute_density_score(train_nz*deltaz,out_nz*deltaz)
    prob_p=average(arr_prob_p,weights=sqrt(train_nz)) # Poisson probability
    ks_s,ks_p=N_ks[0],N_ks[1] # Kolmogorov-Smirnov statistic and probability

    # compute errors from draws (if exists)
    if out_nz_draws is not None:
        out_nz_err=std(out_nz_draws,axis=0)
        out_nz_sig=(out_nz-train_nz)/out_nz_err
        out_nz_prob=2*(1-stats.norm.cdf(abs(out_nz_sig)))
        tprob=average(out_nz_prob[z_sel],weights=train_nz[z_sel])
        tsig=sum(abs(out_nz_sig[z_sel])*train_nz[z_sel]/sum(train_nz[z_sel]))

    # initializing figure
    gs=gridspec.GridSpec(2,1,height_ratios=[4,1])

    # plot N(z)
    subplot(gs[0])
    plot(zgrid,train_nz,color='black',lw=3,label=sample_names[0])
    if out_nz_draws is not None:
        for draw in out_nz_draws:
            plot(zgrid,draw,color='red',lw=0.2,alpha=0.3)
    plot(zgrid,out_nz,color='red',lw=3,label=sample_names[1])
    fill_between(zgrid,train_nz,out_nz,color='yellow')
    yscale('log',noposy='clip')
    xlim(zrange)
    ylims=[1,max([max(train_nz),max(out_nz)])*1.5]
    log_ylims=log10(ylims)
    ylim(ylims)
    legend(fontsize=24)
    xlabel('Redshift')
    ylabel('$N(z)$')
    text(zrange[1]-(zrange[1]-zrange[0])*0.25,10**(log_ylims[1]-(log_ylims[1]-log_ylims[0])*0.27),'Poisson$(\sigma,p)$=('+str(round(N_p,2))+','+str(round(prob_p,2))+')')
    text(zrange[1]-(zrange[1]-zrange[0])*0.25,10**(log_ylims[1]-(log_ylims[1]-log_ylims[0])*0.36),'KS$(\sigma,p)$=('+str(round(ks_s,2))+','+str(round(ks_p,2))+')')
    if out_nz_draws is not None:
        text(zrange[1]-(zrange[1]-zrange[0])*0.25,10**(log_ylims[1]-(log_ylims[1]-log_ylims[0])*0.45),'Error$(\sigma,p)$=('+str(round(tsig,2))+','+str(round(tprob,2))+')')
    tight_layout()

    # plot running Poisson/error fluctuation
    
    subplot(gs[1])
    xlim(zrange)
    xlabel('Redshift')
    ylabel('$\Delta \sigma$')
    plot(zgrid,zeros(len(zgrid)),'k--',lw=2)
    delta_np=sign(out_nz-train_nz)[z_sel]*append(arr_p[0],(arr_p[1:]-arr_p[:-1]))
    fill_between(zgrid[z_sel]+deltaz/2,delta_np,color='yellow',alpha=0.7)
    plot(zgrid[z_sel]+deltaz/2,delta_np,lw=2,color='black')
    ymin,ymax=round(min(delta_np),2),round(max(delta_np),2)
    yticks([ymin,ymax])

    if out_nz_draws is not None:
        sigt=cumsum(out_nz_sig[z_sel]*train_nz[z_sel]/sum(train_nz[z_sel]))
        dt=append(sigt[0],sigt[1:]-sigt[:-1])
        fill_between(zgrid[z_sel]+deltaz/2,dt,color='orange',alpha=0.7)
        plot(zgrid[z_sel]+deltaz/2,dt,lw=2,color='red')
        ymin,ymax=min(round(min(dt),2),ymin),max(round(max(dt),2),ymax)
        yticks([ymin,ymax])

    tight_layout()

    return [N_p,arr_p,arr_prob_p,z_sel],N_ks


def plot_zpoints(plot_title, y, yp, markersize=1.5, limits=[0,6], binwidth=0.05, thresh=10, selection=None, weights=None):
    """
    Plot results from redshift POINT ESTIMATES. To illustrate density scales, 2-D density histograms are used for the majority of the data, while outlying points are plotted individually.

    Keyword arguments:
    plot_title -- plot title
    y -- input values
    yp -- predicted values
    markersize -- size of outlying points
    limits -- scale of x,y axes
    binwidth -- width of 2-D histogram bins
    thresh -- threshold before switching from histogram to outlying points
    selection -- selection array for plotting a subset of objects

    Outputs:
    Identical to func::compute_score.
    """

    cmap=get_cmap('jet')
    cmap.set_bad('white')

    success_sel=(isnan(yp)|(yp<=0))==False # NaNs and z<=0 are counted as failures

    if weights is not None:
        weights=weights
    else:
        weights=ones(len(y))
    
    if selection is not None:
        sel=success_sel&selection&(weights>0.)
    else:
        sel=success_sel&(weights>0.)

    score=compute_score(y[sel],yp[sel],weights=weights[sel])

    # declare binning parameters
    xyrange=[[0,10],[0,10]]
    bins=[arange(xyrange[0][0],xyrange[0][1]+binwidth,binwidth),arange(xyrange[1][0],xyrange[1][1]+binwidth,binwidth)]
    
    # bin data
    xdat,ydat=y[sel],yp[sel]
    hh,locx,locy=histogram2d(xdat,ydat,range=xyrange,bins=bins,weights=weights[sel])
    posx=digitize(xdat,locx)
    posy=digitize(ydat,locy)

    #select points within the histogram
    hhsub=hh[posx-1,posy-1] # values of the histogram where the points are
    xdat1=xdat[(hhsub<thresh)] # low density points (x)
    ydat1=ydat[(hhsub<thresh)] # low density points (y)
    hh[hh<thresh]=NaN # fill the areas with low density by NaNs

    # plot results
    plot(xdat1, ydat1,'.',color='black',markersize=markersize) # outliers/low-density regions
    imshow(flipud(hh.T),cmap='jet',extent=array(xyrange).flatten(),interpolation='none',norm=matplotlib.colors.LogNorm()) # high-density regions

    # establishing the colorbar
    cbar=colorbar()
    cticks=arange(ceil(log10(nanmin(hh.flatten()))/0.25)*0.25,int(log10(nanmax(hh.flatten()))/0.25)*0.25+1e-6,0.25)
    cticklabels=['$10^{'+str(round(i,2))+'}$' for i in cticks]
    cbar.set_ticks(10**cticks)
    cbar.set_ticklabels(cticklabels)

    # plotting 1:1 line+bounds
    plot(array([0,100]),array([0,100]),'k--',lw=3)
    plot(array([0,100]),array([0,100])*1.15+0.15,'k-.',lw=2)
    plot(array([0,100]),array([0,100])*0.85-0.15,'k-.',lw=2)
    title(plot_title,y=1.02)

    # statistics
    Nobj=sum(weights[sel])
    text(1.2*(limits[1]/5.0),4.7*(limits[1]/5.0),"$N$: "+str(int(Nobj))+" ("+str(round(Nobj*1.0/sum(weights[success_sel]),3))+")",fontsize=18,color='black')
    text(1.2*(limits[1]/5.0),4.5*(limits[1]/5.0),"$\Delta z^\prime$ (mean): "+str(round(score[0][2],4)*100)+"%",fontsize=18,color='black')
    text(1.2*(limits[1]/5.0),4.3*(limits[1]/5.0),"$\Delta z^\prime$ (med): "+str(round(score[1][2],4)*100)+"%",fontsize=18,color='black')
    text(1.2*(limits[1]/5.0),4.1*(limits[1]/5.0),"$\sigma_{\Delta z^\prime}$ (MAD): "+str(round(score[1][3],4)*100)+"%",fontsize=18,color='black')
    text(1.2*(limits[1]/5.0),3.9*(limits[1]/5.0),"$f_{cat}$: "+str(round(score[4],4)*100)+"%",fontsize=18,color='black')

    # miscallaneous
    xlabel('Input')
    ylabel('Output')
    xlim(limits)
    ylim(limits)

    return score


def plot_zpdfstack(filt, zpdf, zgrid, lz_idx, lze_idx, rdict, sel=None, weights=None, limits=[0,6,1.0], pdf_thresh=1e-3, plot_thresh=1., zres=1e-3, boxcar=1):
    """
    Plot 2-D P(z) vs redshift.

    Keyword arguments:
    filt -- filter object
    zpdf -- redshift PDFs
    zgrid -- redshift grid PDFs are evaluated on
    cell_lz_idx -- log(1+z) discretized indices
    cell_lze_idx -- log(1+z) dictionary indoces
    rdict -- redshift dictionary
    sel -- selection array
    limits -- [low,high,delta] parameters for plotting
    pdf_thresh -- input to func::pdf_kde_wt_dict
    plot_thresh -- minimum threshold for plotting stacked PDFs
    zres -- resolution input to func::pdfs_resample
    boxcar -- width of boxcar for smoothing running mean, median, etc.

    Outputs:
    2d_stack,np,zpoints
    2d_stack -- 2-D stacked PDF
    np -- number density computed along input redshift axis
    zpoints -- point estimates from func::pdfs_summary_statistics computed along input redshift axis
    """

    Np,Nz=len(rdict.lzgrid_highres),len(zgrid)
    Nobj=len(zpdf)
    temp_stack=zeros((Np,Nz)) # 2-D P(z) grid

    if sel is None:
        sel=ones(Nobj,dtype='bool')

    if weights is None:
        weights=ones(Nobj,dtype='float32')

    # compute stack
    count=0
    for i in arange(Nobj)[sel]:
        if count%5000==0: sys.stderr.write(str(count)+' ')
        count+=1
        tzpdf=zpdf[i] # redshift pdf
        tsel=tzpdf>max(tzpdf)*pdf_thresh # probability threshold cut
        x_idx,x_cent=lze_idx[i],lz_idx[i]
        x_bound=rdict.lze_width[x_idx] # dictionary entry, location, and width
        tstack=rdict.lze_dict[x_idx][:,None]*tzpdf[tsel] # 2-D pdf
        temp_stack[x_cent-x_bound:x_cent+x_bound+1,tsel]+=tstack*weights[i] # stack 2-D pdf

    zpoints=pdfs_summary_statistics(zgrid,temp_stack/trapz(temp_stack,zgrid)[:,None],zres) # compute summary statistics
    for i in zpoints:
        i[i==zgrid[0]]=NaN

    # converting from log to linear space
    temp_stack=temp_stack[rdict.zmin_idx_highres:rdict.zmax_idx_highres:int(rdict.res)]/rdict.znorm[:,None] # reducing resolution
    prob=interpolate.interp1d(rdict.zgrid,trapz(temp_stack,zgrid))(zgrid) # running pdf
    temp_stack=swapaxes(pdfs_resample(rdict.zgrid,swapaxes(temp_stack,0,1),zgrid),0,1) # resampling to linear redshift grid
    temp_stack*=prob[None,:] # re-normalizing
    temp_stack=ma.array(temp_stack,mask=temp_stack<plot_thresh) # mask array

    # plot results
    zgrid_highres=exp(rdict.lzgrid_highres)-1
    imshow(swapaxes(temp_stack,0,1),origin='lower',aspect='auto',norm=matplotlib.colors.LogNorm(vmin=None, vmax=None),extent=(zgrid[0],zgrid[-1],zgrid[0],zgrid[-1]))
    colorbar(label='PDF')
    plot(array([0,100]),array([0,100]),'k--',lw=3) # 1:1 relation
    plot(array([0,100]),array([0,100])*1.15+0.15,'k-.',lw=2) # +15% bound
    plot(array([0,100]),array([0,100])*0.85-0.15,'k-.',lw=2) # -15% bound 
    plot(convolve(zgrid_highres[isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[0][isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),color='black',lw=2) # mean
    plot(convolve(zgrid_highres[isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[1][isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',lw=2) # median
    plot(convolve(zgrid_highres[isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[3][isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2) # lower 68% CI
    plot(convolve(zgrid_highres[isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[4][isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2) # upper 68% CI
    plot(convolve(zgrid_highres[isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[5][isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2) # lower 95% CI
    plot(convolve(zgrid_highres[isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[6][isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2) # upper 95% CI
    xticks(arange(limits[0],limits[1]+limits[2],limits[2]))
    xlim([limits[0],limits[1]])
    yticks(arange(limits[0],limits[1]+limits[2],limits[2]))
    ylim([limits[0],limits[1]])
    xlabel('Redshift (input)')
    ylabel('Redshift (output)')
    tight_layout()

    return temp_stack,prob,zpoints


def plot_pdfstack(filt, zpdf, zgrid, p_idx, pe_idx, pdict, pparams, pname, sel=None, weights=None, yparams=[0,6,1.0], pdf_thresh=1e-3, plot_thresh=1., zres=1e-3, boxcar=1):
    """
    Plot 2-D P(z) vs input parameter dictionary.

    Keyword arguments:
    filt -- filter object
    zpdf -- redshift PDFs
    zgrid -- redshift grid PDFs are evaluated on
    p_idx -- parameter discretized indices
    pe_idx -- parameter dictionary indices
    pdict -- parameter dictionary
    pparams -- as limits (see func::plot_zpdfstack), but for the parameter of interest
    pname -- parameter name
    See func::plot_zpdfstack for additional inputs.

    Outputs:
    2d_stack,np,zpoints
    2d_stack -- 2-D stacked PDF
    np -- number density computed along input parameter axis
    zpoints -- point estimates from func::pdfs_summary_statistics computed along input parameter axis
    """

    Np,Nz=len(pdict.grid),len(zgrid)
    Nobj=len(zpdf)
    temp_stack=zeros((Np,Nz)) # 2-D P(z) grid

    if sel is None:
        sel=ones(Nobj,dtype='bool')

    if weights is None:
        weights=ones(Nobj,dtype='float32')

    # compute stack
    count=0
    for i in arange(Nobj)[sel]:
        if count%5000==0: sys.stderr.write(str(count)+' ')
        count+=1
        tzpdf=zpdf[i] # redshift pdf
        tsel=tzpdf>max(tzpdf)*pdf_thresh # probability threshold cut
        x_idx,x_cent=pe_idx[i],p_idx[i]
        x_bound=pdict.sig_width[x_idx] # dictionary entry, location, and width
        tstack=pdict.sig_dict[x_idx][:,None]*tzpdf[tsel] # 2-D pdf
        temp_stack[x_cent-x_bound:x_cent+x_bound+1,tsel]+=tstack # stack 2-D pdf

    # truncate array
    ylow,yhigh=argmin(abs(pdict.grid-pparams[0])),argmin(abs(pdict.grid-pparams[1]))
    temp_stack=temp_stack[ylow:yhigh+1]
    zpoints=pdfs_summary_statistics(zgrid,temp_stack/trapz(temp_stack,zgrid)[:,None],zres) # compute summary statistics
    for i in zpoints:
        i[i==zgrid[0]]=NaN

    prob=trapz(temp_stack,zgrid) # running pdf
    temp_stack=ma.array(temp_stack,mask=temp_stack<plot_thresh) # mask array

    # plot results
    pgrid=pdict.grid[ylow:yhigh+1]
    imshow(swapaxes(temp_stack,0,1),origin='lower',aspect='auto',norm=matplotlib.colors.LogNorm(vmin=None, vmax=None),extent=(pparams[0],pparams[1],zgrid[0],zgrid[-1]))
    colorbar(label='PDF')
    plot(convolve(pgrid[isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[0][isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),color='black',lw=2) # mean
    plot(convolve(pgrid[isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[1][isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',lw=2) # median
    plot(convolve(pgrid[isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[3][isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2) # lower 68% CI
    plot(convolve(pgrid[isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[4][isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2) # upper 68% CI
    plot(convolve(pgrid[isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[5][isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2) # lower 95% CI
    plot(convolve(pgrid[isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[6][isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2) # upper 95% CI
    xticks(arange(pparams[0],pparams[1]+pparams[2],pparams[2]))
    xlim([pparams[0],pparams[1]])
    yticks(arange(yparams[0],yparams[1]+yparams[2],yparams[2]))
    ylim([yparams[0],yparams[1]])
    xlabel(pname)
    ylabel('Redshift (output)')
    tight_layout()

    return temp_stack,prob,zpoints


def plot_dpdfstack(filt, zpdf, zgrid, z, p_idx, pe_idx, pdict, pparams, pname, sel=None, weights=None, yparams=[-0.5,0.5,0.1], ybins=[-1.0,1.0,1e-2], pdf_thresh=1e-3, plot_thresh=1., zres=1e-3, boxcar=1):
    """
    Plot 2-D [P(z)-z]/(1+z) (redshift dispersion) vs input parameter dictionary.

    Keyword arguments:
    filt -- filter object
    zpdf -- redshift PDFs
    zgrid -- redshift grid PDFs are evaluated on
    p_idx -- parameter discretized indices
    pe_idx -- parameter dictionary indices
    pdict -- parameter dictionary
    pparams -- as limits (see func::plot_zpdfstack), but for the parameter of interest
    pname -- parameter name
    ybins -- binning used when computing redshift dispersion
    See func::plot_zpdfstack for additional inputs.

    Outputs:
    2d_stack,np,zpoints
    2d_stack -- 2-D stacked PDF
    np -- number density computed along input parameter axis
    zpoints -- point estimates from func::pdfs_summary_statistics computed along input parameter axis
    """

    # error distribution (PDF)
    zdisp_bins=arange(ybins[0],ybins[1],ybins[2])
    zdisp_grid=(zdisp_bins[1:]+zdisp_bins[:-1])/2.0

    Np,Ndisp=len(pdict.grid),len(zdisp_bins)-1
    Nobj=len(zpdf)
    temp_stack=zeros((Np,Ndisp)) # 2-D P(z) grid

    if sel is None:
        sel=ones(Nobj,dtype='bool')

    if weights is None:
        weights=ones(Nobj,dtype='float32')

    count=0
    for i in arange(Nobj)[sel]:
        if count%5000==0: sys.stderr.write(str(count)+' ')
        count+=1
        tzpdf=zpdf[i]
        tsel=tzpdf>max(tzpdf)*pdf_thresh # probability threshold cut
        x_idx,x_cent=pe_idx[i],p_idx[i] # dictionary entry and location
        x_bound=pdict.sig_width[x_idx] # dictionary width
        pstack=histogram((zgrid[tsel]-z[i])/(1+z[i]),zdisp_bins,weights=tzpdf[tsel])[0] # d(pdf) stack
        psel=pstack>max(pstack)*pdf_thresh
        tstack=pdict.sig_dict[x_idx][:,None]*pstack[psel]
        temp_stack[x_cent-x_bound:x_cent+x_bound+1,psel]+=tstack*weights[i]

    # truncate array
    ylow,yhigh=argmin(abs(pdict.grid-pparams[0])),argmin(abs(pdict.grid-pparams[1]))
    temp_stack=temp_stack[ylow:yhigh+1]
    zpoints=pdfs_summary_statistics(zdisp_grid,temp_stack/trapz(temp_stack,zdisp_grid)[:,None],zres) # compute summary statistics
    for i in zpoints:
        i[i==zdisp_grid[0]]=NaN

    prob=trapz(temp_stack,zdisp_grid) # running pdf
    temp_stack=ma.array(temp_stack,mask=temp_stack<plot_thresh) # mask array

    # plot results
    pgrid=pdict.grid[ylow:yhigh+1]
    imshow(swapaxes(temp_stack,0,1),origin='lower',aspect='auto',norm=matplotlib.colors.LogNorm(vmin=None, vmax=None),extent=(pparams[0],pparams[1],zdisp_grid[0],zdisp_grid[-1]))
    colorbar(label='PDF')
    plot(array([-100,100]),array([0,0]),'k--',lw=3)
    plot(array([-100,100]),[0.15,0.15],'k-.',lw=2)
    plot(array([-100,100]),[-0.15,-0.15],'k-.',lw=2)
    plot(convolve(pgrid[isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[0][isnan(zpoints[0])==False],ones(boxcar)/float(boxcar),'valid'),color='black',lw=2) # mean
    plot(convolve(pgrid[isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[1][isnan(zpoints[1])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',lw=2) # median
    plot(convolve(pgrid[isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[3][isnan(zpoints[3])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2)
    plot(convolve(pgrid[isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[4][isnan(zpoints[4])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='--',lw=2)
    plot(convolve(pgrid[isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[5][isnan(zpoints[5])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2)
    plot(convolve(pgrid[isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),convolve(zpoints[6][isnan(zpoints[6])==False],ones(boxcar)/float(boxcar),'valid'),color='darkviolet',linestyle='-.',lw=2)    
    xticks(arange(pparams[0],pparams[1]+pparams[2],pparams[2]))
    xlim([pparams[0],pparams[1]])
    yticks(arange(yparams[0],yparams[1]+yparams[2],yparams[2]))
    ylim([yparams[0],yparams[1]])
    xlabel(pname)
    ylabel('$\Delta z/(1+z)$ (output)')
    tight_layout()

    return temp_stack,prob,zpoints
